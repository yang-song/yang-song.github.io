<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://yang-song.net/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yang-song.net/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-19T00:36:29+00:00</updated><id>https://yang-song.net/feed.xml</id><title type="html">Yang Song</title><subtitle>The personal website of Yang Song. </subtitle><entry><title type="html">a post with redirect</title><link href="https://yang-song.net/blog/2021/redirect/" rel="alternate" type="text/html" title="a post with redirect"/><published>2021-07-04T17:39:00+00:00</published><updated>2021-07-04T17:39:00+00:00</updated><id>https://yang-song.net/blog/2021/redirect</id><content type="html" xml:base="https://yang-song.net/blog/2021/redirect/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[you can also redirect to assets like pdf]]></summary></entry><entry><title type="html">a distill-style blog post</title><link href="https://yang-song.net/blog/2021/distill/" rel="alternate" type="text/html" title="a distill-style blog post"/><published>2021-05-22T00:00:00+00:00</published><updated>2021-05-22T00:00:00+00:00</updated><id>https://yang-song.net/blog/2021/distill</id><content type="html" xml:base="https://yang-song.net/blog/2021/distill/"><![CDATA[<p><strong>NOTE:</strong> Citations, footnotes, and code blocks do not display correctly in the dark mode since distill does not support the dark mode by default. If you are interested in correctly adding dark mode support for distill, please open <a href="https://github.com/alshedivat/al-folio/discussions">a discussion</a> and let us know.</p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <hr/> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>Syntax highlighting is provided within <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> tags. An example of inline code snippets: <code class="language-plaintext highlighter-rouge">&lt;d-code language="html"&gt;let x = 10;&lt;/d-code&gt;</code>. For larger blocks of code, add a <code class="language-plaintext highlighter-rouge">block</code> attribute:</p> <d-code block="" language="javascript"> var x = 25; function(x) { return x * x; } </d-code> <p><strong>Note:</strong> <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> blocks do not look well in the dark mode. You can always use the default code-highlight using the <code class="language-plaintext highlighter-rouge">highlight</code> liquid tag:</p> <figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">var</span> <span class="nx">x</span> <span class="o">=</span> <span class="mi">25</span><span class="p">;</span>
<span class="kd">function</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">return</span> <span class="nx">x</span> <span class="o">*</span> <span class="nx">x</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code> sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item ⋅⋅* Unordered sub-list.</li> <li>Actual numbers don’t matter, just that it’s a number ⋅⋅1. Ordered sub-list</li> <li>And another item.</li> </ol> <p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅ ⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅ ⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</p> <ul> <li>Unordered list can use asterisks</li> <li>Or minuses</li> <li>Or pluses</li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="k">print</span> <span class="n">s</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting. 
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[an example of a distill-style blog post and main elements]]></summary></entry><entry><title type="html">Generative Modeling by Estimating Gradients of the Data Distribution</title><link href="https://yang-song.net/blog/2021/score/" rel="alternate" type="text/html" title="Generative Modeling by Estimating Gradients of the Data Distribution"/><published>2021-05-05T15:12:00+00:00</published><updated>2021-05-05T15:12:00+00:00</updated><id>https://yang-song.net/blog/2021/score</id><content type="html" xml:base="https://yang-song.net/blog/2021/score/"><![CDATA[<d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction"> Introduction </a></div> <div><a href="#the-score-function-score-based-models-and-score-matching">The score function, score-based models, and score matching</a></div> <div><a href="#langevin-dynamics">Langevin dynamics</a></div> <div><a href="#naive-score-based-generative-modeling-and-its-pitfalls">Naive score-based generative modeling and its pitfalls</a></div> <div><a href="#score-based-generative-modeling-with-multiple-noise-perturbations">Score-based generative modeling with multiple noise perturbations</a></div> <div><a href="#score-based-generative-modeling-with-stochastic-differential-equations-sdes">Score-based generative modeling with stochastic differential equations (SDEs)</a> </div> <ul> <li><a href="#perturbing-data-with-an-sde">Perturbing data with an SDE</a></li> <li><a href="#reversing-the-sde-for-sample-generation">Reversing the SDE for sample generation</a></li> <li><a href="#estimating-the-reverse-sde-with-score-based-models-and-score-matching">Estimating the reverse SDE with score-based models and score matching</a></li> <li><a href="#how-to-solve-the-reverse-sde"> How to solve the reverse SDE </a></li> <li><a href="#probability-flow-ode">Probability flow ODE</a></li> <li><a href="#controllable-generation-for-inverse-problem-solving">Controllable generation for inverse problem solving</a></li> </ul> <div><a href="#connection-to-diffusion-models-and-others">Connection to diffusion models and others</a></div> <div><a href="#concluding-remarks">Concluding remarks</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Existing generative modeling techniques can largely be grouped into two categories based on how they represent probability distributions.</p> <ol> <li><strong>likelihood-based models</strong>, which directly learn the distribution’s probability density (or mass) function via (approximate) maximum likelihood. Typical likelihood-based models include autoregressive models <d-cite key="larochelle2011neural,germain2015made,van2016pixel"></d-cite>, normalizing flow models <d-cite key="dinh2014nice,dinh2016density"></d-cite>, energy-based models (EBMs)<d-cite key="lecun2006tutorial,song2021train"></d-cite>, and variational auto-encoders (VAEs) <d-cite key="kingma2013auto,rezende2014stochastic"></d-cite>.</li> <li><strong>implicit generative models</strong> <d-cite key="mohamed2016learning"></d-cite>, where the probability distribution is implicitly represented by a model of its sampling process. The most prominent example is generative adversarial networks (GANs) <d-cite key="goodfellow2014generative"></d-cite>, where new samples from the data distribution are synthesized by transforming a random Gaussian vector with a neural network.</li> </ol> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/likelihood_based_models.png"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Bayesian networks, Markov random fields (MRF), autoregressive models, and normalizing flow models are all examples of likelihood-based models. All these models represent the probability density or mass function of a distribution. </figcaption> </div> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/implicit_models.png"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> GAN is an example of implicit models. It implicitly represents a distribution over all objects that can be produced by the generator network. </figcaption> </div> <p>Likelihood-based models and implicit generative models, however, both have significant limitations. Likelihood-based models either require strong restrictions on the model architecture to ensure a tractable normalizing constant for likelihood computation, or must rely on surrogate objectives to approximate maximum likelihood training. Implicit generative models, on the other hand, often require adversarial training, which is notoriously unstable <d-cite key="salimans2016improved"></d-cite> and can lead to mode collapse <d-cite key="metz2017unrolled"></d-cite>.</p> <p>In this blog post, I will introduce another way to represent probability distributions that may circumvent several of these limitations. The key idea is to model <em>the gradient of the log probability density function</em>, a quantity often known as the (Stein) <strong>score function</strong> <d-cite key="chwialkowski16,liu2016kernelized"></d-cite>. Such <strong>score-based models</strong> are not required to have a tractable normalizing constant, and can be directly learned by <strong>score matching</strong> <d-cite key="hyvarinen2005estimation,vincent2011connection"></d-cite>.</p> <div class="l-page"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/score_contour.jpg" style="display: block; width: max(30%, 200px); margin-left: auto; margin-right: auto;"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Score function (the vector field) and density function (contours) of a mixture of two Gaussians. </figcaption> </div> <p>Score-based models have achieved state-of-the-art performance on many downstream tasks and applications. These tasks include, among others, image generation <d-cite key="song2019generative,song2020improved,ho2020denoising,song2021scorebased,dhariwal2021diffusion,ho2021cascaded"></d-cite> (Yes, better than GANs!), audio synthesis <d-cite key="chen2021wavegrad,kong2021diffwave,popov2021grad"></d-cite>, shape generation<d-cite key="ShapeGF"></d-cite>, and music generation<d-cite key="mittal2021symbolic"></d-cite>. Moreover, score-based models have connections to <a href="https://blog.evjang.com/2018/01/nf1.html">normalizing flow models</a>, therefore allowing exact likelihood computation and representation learning. Additionally, modeling and estimating scores facilitates <a href="https://en.wikipedia.org/wiki/Inverse_problem#:~:text=An%20inverse%20problem%20in%20science,measurements%20of%20its%20gravity%20field">inverse problem</a> solving, with applications such as image inpainting <d-cite key="song2019generative,song2021scorebased"></d-cite>, image colorization <d-cite key="song2021scorebased"></d-cite>, <a href="https://en.wikipedia.org/wiki/Compressed_sensing">compressive sensing</a>, and medical image reconstruction (e.g., CT, MRI) <d-cite key="jalal2021robust"></d-cite>.</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/ffhq_samples.jpg"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> 1024 x 1024 samples generated from score-based models <d-cite key="song2021scorebased"></d-cite> </figcaption> </div> <p>This post aims to show you the motivation and intuition of score-based generative modeling, as well as its basic concepts, properties and applications.</p> <h2 id="the-score-function-score-based-models-and-score-matching">The score function, score-based models, and score matching</h2> <p>Suppose we are given a dataset \(\{\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_N\}\), where each point is drawn independently from an underlying data distribution \(p(\mathbf{x})\). Given this dataset, the goal of generative modeling is to fit a model to the data distribution such that we can synthesize new data points at will by sampling from the distribution.</p> <p>In order to build such a generative model, we first need a way to represent a probability distribution. One such way, as in likelihood-based models, is to directly model the <a href="https://en.wikipedia.org/wiki/Probability_density_function">probability density function</a> (p.d.f.) or <a href="https://en.wikipedia.org/wiki/Probability_mass_function">probability mass function</a> (p.m.f.). Let \(f_\theta(\mathbf{x}) \in \mathbb{R}\) be a real-valued function parameterized by a learnable parameter \(\theta\). We can define a p.d.f. <d-footnote> Hereafter we only consider probability density functions. Probability mass functions are similar. </d-footnote> via \begin{align} p_\theta(\mathbf{x}) = \frac{e^{-f_\theta(\mathbf{x})}}{Z_\theta}, \label{ebm} \end{align} where \(Z_\theta &gt; 0\) is a normalizing constant dependent on \(\theta\), such that \(\int p_\theta(\mathbf{x}) \textrm{d} \mathbf{x} = 1\). Here the function \(f_\theta(\mathbf{x})\) is often called an unnormalized probabilistic model, or energy-based model <d-cite key="song2021train"></d-cite>.</p> <p>We can train \(p_\theta(\mathbf{x})\) by maximizing the log-likelihood of the data \begin{align} \max_\theta \sum_{i=1}^N \log p_\theta(\mathbf{x}_i). \label{mle} \end{align} However, equation \eqref{mle} requires \(p_\theta(\mathbf{x})\) to be a normalized probability density function. This is undesirable because in order to compute \(p_\theta(\mathbf{x})\), we must evaluate the normalizing constant \(Z_\theta\)—a typically intractable quantity for any general \(f_\theta(\mathbf{x})\). Thus to make maximum likelihood training feasible, likelihood-based models must either restrict their model architectures (e.g., causal convolutions in autoregressive models, invertible networks in normalizing flow models) to make \(Z_\theta\) tractable, or approximate the normalizing constant (e.g., variational inference in VAEs, or MCMC sampling used in contrastive divergence<d-cite key="hinton2002training"></d-cite>) which may be computationally expensive.</p> <p>By modeling the score function instead of the density function, we can sidestep the difficulty of intractable normalizing constants. The <strong>score function</strong> of a distribution \(p(\mathbf{x})\) is defined as \begin{equation} \nabla_\mathbf{x} \log p(\mathbf{x}), \notag \end{equation} and a model for the score function is called a <strong>score-based model</strong> <d-cite key="song2019generative"></d-cite>, which we denote as \(\mathbf{s}_\theta(\mathbf{x})\). The score-based model is learned such that \(\mathbf{s}_\theta(\mathbf{x}) \approx \nabla_\mathbf{x} \log p(\mathbf{x})\), and can be parameterized without worrying about the normalizing constant. For example, we can easily parameterize a score-based model with the energy-based model defined in equation \eqref{ebm} , via</p> \[\begin{equation} \mathbf{s}_\theta (\mathbf{x}) = \nabla_{\mathbf{x}} \log p_\theta (\mathbf{x} ) = -\nabla_{\mathbf{x}} f_\theta (\mathbf{x}) - \underbrace{\nabla_\mathbf{x} \log Z_\theta}_{=0} = -\nabla_\mathbf{x} f_\theta(\mathbf{x}). \end{equation}\] <p>Note that the score-based model \(\mathbf{s}_\theta(\mathbf{x})\) is independent of the normalizing constant \(Z_\theta\) ! This significantly expands the family of models that we can tractably use, since we don’t need any special architectures to make the normalizing constant tractable.</p> <div class="row l-body"> <div class="col-sm"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/ebm.gif"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Parameterizing probability density functions. No matter how you change the model family and parameters, it has to be normalized (area under the curve must integrate to one). </figcaption> </div> <div class="col-sm"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/score.gif"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px">Parameterizing score functions. No need to worry about normalization. </figcaption> </div> </div> <p>Similar to likelihood-based models, we can train score-based models by minimizing the <strong>Fisher divergence</strong> <d-footnote>Fisher divergence is typically between two distributions p and q, defined as $$\begin{equation} \mathbb{E}_{p(\mathbf{x})}[\| \nabla_\mathbf{x} \log p(\mathbf{x}) - \nabla_\mathbf{x}\log q(\mathbf{x}) \|_2^2]. \end{equation}$$ Here we slightly abuse the term as the name of a closely related expression for score-based models. </d-footnote> between the model and the data distributions, defined as \(\begin{equation} \mathbb{E}_{p(\mathbf{x})}[\| \nabla_\mathbf{x} \log p(\mathbf{x}) - \mathbf{s}_\theta(\mathbf{x}) \|_2^2]\label{fisher} \end{equation}\)</p> <p>Intuitively, the Fisher divergence compares the squared \(\ell_2\) distance between the ground-truth data score and the score-based model. Directly computing this divergence, however, is infeasible because it requires access to the unknown data score \(\nabla_\mathbf{x} \log p(\mathbf{x})\). Fortunately, there exists a family of methods called <strong>score matching</strong> <d-footnote>Commonly used score matching methods include denoising score matching <d-cite key="vincent2011connection"></d-cite> and sliced score matching <d-cite key="song2019sliced"></d-cite>. Here is an introduction to <a href="/blog/2019/ssm/">score matching and sliced score matching</a>. </d-footnote> <d-cite key="hyvarinen2005estimation,vincent2011connection,song2019sliced"></d-cite> that minimize the Fisher divergence without knowledge of the ground-truth data score. Score matching objectives can directly be estimated on a dataset and optimized with stochastic gradient descent, analogous to the log-likelihood objective for training likelihood-based models (with known normalizing constants). We can train the score-based model by minimizing a score matching objective, <strong>without requiring adversarial optimization</strong>.</p> <p>Additionally, using the score matching objective gives us a considerable amount of modeling flexibility. The Fisher divergence itself does not require \(\mathbf{s}_\theta(\mathbf{x})\) to be an actual score function of any normalized distribution—it simply compares the \(\ell_2\) distance between the ground-truth data score and the score-based model, with no additional assumptions on the form of \(\mathbf{s}_\theta(\mathbf{x})\). In fact, the only requirement on the score-based model is that it should be a vector-valued function with the same input and output dimensionality, which is easy to satisfy in practice.</p> <p>As a brief summary, we can represent a distribution by modeling its score function, which can be estimated by training a score-based model of free-form architectures with score matching.</p> <h2 id="langevin-dynamics">Langevin dynamics</h2> <p>Once we have trained a score-based model \(\mathbf{s}_\theta(\mathbf{x}) \approx \nabla_\mathbf{x} \log p(\mathbf{x})\), we can use an iterative procedure called <a href="https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm"><strong>Langevin dynamics</strong></a><d-cite key="parisi1981correlation,grenander1994representations"></d-cite> to draw samples from it.</p> <p>Langevin dynamics provides an MCMC procedure to sample from a distribution \(p(\mathbf{x})\) using only its score function \(\nabla_\mathbf{x} \log p(\mathbf{x})\). Specifically, it initializes the chain from an arbitrary prior distribution \(\mathbf{x}_0 \sim \pi(\mathbf{x})\), and then iterates the following</p> \[\begin{align} \mathbf{x}_{i+1} \gets \mathbf{x}_i + \epsilon \nabla_\mathbf{x} \log p(\mathbf{x}) + \sqrt{2\epsilon}~ \mathbf{z}_i, \quad i=0,1,\cdots, K, \label{langevin} \end{align}\] <p>where \(\mathbf{z}_i \sim \mathcal{N}(0, I)\). When \(\epsilon \to 0\) and \(K \to \infty\), \(\mathbf{x}_K\) obtained from the procedure in \eqref{langevin} converges to a sample from \(p(\mathbf{x})\) under some regularity conditions. In practice, the error is negligible when \(\epsilon\) is sufficiently small and \(K\) is sufficiently large.</p> <div class="l-page"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/langevin.gif" style="display: block; width: max(30%, 300px); margin-left: auto; margin-right: auto;"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;">Using Langevin dynamics to sample from a mixture of two Gaussians. </figcaption> </div> <p>Note that Langevin dynamics accesses \(p(\mathbf{x})\) only through \(\nabla_\mathbf{x} \log p(\mathbf{x})\). Since \(\mathbf{s}_\theta(\mathbf{x}) \approx \nabla_\mathbf{x} \log p(\mathbf{x})\), we can produce samples from our score-based model \(\mathbf{s}_\theta(\mathbf{x})\) by plugging it into equation \eqref{langevin}.</p> <h2 id="naive-score-based-generative-modeling-and-its-pitfalls">Naive score-based generative modeling and its pitfalls</h2> <p>So far, we’ve discussed how to train a score-based model with score matching, and then produce samples via Langevin dynamics. However, this naive approach has had limited success in practice—we’ll talk about some pitfalls of score matching that received little attention in prior works<d-cite key="song2019generative"></d-cite>.</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/smld.jpg"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Score-based generative modeling with score matching + Langevin dynamics. </figcaption> </div> <p>The key challenge is the fact that the estimated score functions are inaccurate in low density regions, where few data points are available for computing the score matching objective. This is expected as score matching minimizes the Fisher divergence</p> \[\mathbb{E}_{p(\mathbf{x})}[\| \nabla_\mathbf{x} \log p(\mathbf{x}) - \mathbf{s}_\theta(\mathbf{x}) \|_2^2] = \int p(\mathbf{x}) \| \nabla_\mathbf{x} \log p(\mathbf{x}) - \mathbf{s}_\theta(\mathbf{x}) \|_2^2 \mathrm{d}\mathbf{x}.\] <p>Since the \(\ell_2\) differences between the true data score function and score-based model are weighted by \(p(\mathbf{x})\), they are largely ignored in low density regions where \(p(\mathbf{x})\) is small. This behavior can lead to subpar results, as illustrated by the figure below:</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/pitfalls.jpg"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Estimated scores are only accurate in high density regions. </figcaption> </div> <p>When sampling with Langevin dynamics, our initial sample is highly likely in low density regions when data reside in a high dimensional space. Therefore, having an inaccurate score-based model will derail Langevin dynamics from the very beginning of the procedure, preventing it from generating high quality samples that are representative of the data.</p> <h2 id="score-based-generative-modeling-with-multiple-noise-perturbations">Score-based generative modeling with multiple noise perturbations</h2> <p>How can we bypass the difficulty of accurate score estimation in regions of low data density? Our solution is to <strong>perturb</strong> data points with noise and train score-based models on the noisy data points instead. When the noise magnitude is sufficiently large, it can populate low data density regions to improve the accuracy of estimated scores. For example, here is what happens when we perturb a mixture of two Gaussians perturbed by additional Gaussian noise.</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/single_noise.jpg"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Estimated scores are accurate everywhere for the noise-perturbed data distribution due to reduced low data density regions. </figcaption> </div> <p>Yet another question remains: how do we choose an appropriate noise scale for the perturbation process? Larger noise can obviously cover more low density regions for better score estimation, but it over-corrupts the data and alters it significantly from the original distribution. Smaller noise, on the other hand, causes less corruption of the original data distribution, but does not cover the low density regions as well as we would like.</p> <p>To achieve the best of both worlds, we use multiple scales of noise perturbations simultaneously <d-cite key="song2019generative,song2020improved"></d-cite>. Suppose we always perturb the data with isotropic Gaussian noise, and let there be a total of \(L\) increasing standard deviations \(\sigma_1 &lt; \sigma_2 &lt; \cdots &lt; \sigma_L\). We first perturb the data distribution \(p(\mathbf{x})\) with each of the Gaussian noise \(\mathcal{N}(0, \sigma_i^2 I), i=1,2,\cdots,L\) to obtain a noise-perturbed distribution</p> \[p_{\sigma_i}(\mathbf{x}) = \int p(\mathbf{y}) \mathcal{N}(\mathbf{x}; \mathbf{y}, \sigma_i^2 I) \mathrm{d} \mathbf{y}.\] <p>Note that we can easily draw samples from \(p_{\sigma_i}(\mathbf{x})\) by sampling \(\mathbf{x} \sim p(\mathbf{x})\) and computing \(\mathbf{x} + \sigma_i \mathbf{z}\), with \(\mathbf{z} \sim \mathcal{N}(0, I)\).</p> <p>Next, we estimate the score function of each noise-perturbed distribution, \(\nabla_\mathbf{x} \log p_{\sigma_i}(\mathbf{x})\), by training a <strong>Noise Conditional Score-Based Model</strong> \(\mathbf{s}_\theta(\mathbf{x}, i)\) (also called a Noise Conditional Score Network, or NCSN<d-cite key="song2019generative,song2020improved,song2021scorebased"></d-cite>, when parameterized with a neural network) with score matching, such that \(\mathbf{s}_\theta(\mathbf{x}, i) \approx \nabla_\mathbf{x} \log p_{\sigma_i}(\mathbf{x})\) for all \(i= 1, 2, \cdots, L\).</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/multi_scale.jpg"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> We apply multiple scales of Gaussian noise to perturb the data distribution (<strong>first row</strong>), and jointly estimate the score functions for all of them (<strong>second row</strong>).</figcaption> </div> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/duoduo.jpg"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px"> Perturbing an image with multiple scales of Gaussian noise.</figcaption> </div> <p>The training objective for \(\mathbf{s}_\theta(\mathbf{x}, i)\) is a weighted sum of Fisher divergences for all noise scales. In particular, we use the objective below:</p> \[\begin{equation} \sum_{i=1}^L \lambda(i) \mathbb{E}_{p_{\sigma_i}(\mathbf{x})}[\| \nabla_\mathbf{x} \log p_{\sigma_i}(\mathbf{x}) - \mathbf{s}_\theta(\mathbf{x}, i) \|_2^2],\label{ncsn_obj} \end{equation}\] <p>where \(\lambda(i) \in \mathbb{R}_{&gt;0}\) is a positive weighting function, often chosen to be \(\lambda(i) = \sigma_i^2\). The objective \eqref{ncsn_obj} can be optimized with score matching, exactly as in optimizing the naive (unconditional) score-based model \(\mathbf{s}_\theta(\mathbf{x})\).</p> <p>After training our noise-conditional score-based model \(\mathbf{s}_\theta(\mathbf{x}, i)\), we can produce samples from it by running Langevin dynamics for \(i = L, L-1, \cdots, 1\) in sequence. This method is called <strong>annealed Langevin dynamics</strong> (defined by Algorithm 1 in <d-cite key="song2019generative"></d-cite>, and improved by <d-cite key="song2020improved,jolicoeur-martineau2021adversarial"></d-cite>), since the noise scale \(\sigma_i\) decreases (anneals) gradually over time.</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/ald.gif"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Annealed Langevin dynamics combine a sequence of Langevin chains with gradually decreasing noise scales. </figcaption> </div> <div class="container"> <div class="row l-body"> <div class="col-sm"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/celeba_large.gif"/> </div> <div class="col-sm"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/cifar10_large.gif"/> </div> </div> <div class="row l-body"> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Annealed Langevin dynamics for the Noise Conditional Score Network (NCSN) model (from ref.<d-cite key="song2019generative"></d-cite>) trained on CelebA (<strong>left</strong>) and CIFAR-10 (<strong>right</strong>). We can start from unstructured noise, modify images according to the scores, and generate nice samples. The method achieved state-of-the-art Inception score on CIFAR-10 at its time. </figcaption> </div> </div> <p>Here are some practical recommendations for tuning score-based generative models with multiple noise scales:</p> <ul> <li> <p>Choose \(\sigma_1 &lt; \sigma_2 &lt; \cdots &lt; \sigma_L\) as a <a href="https://en.wikipedia.org/wiki/Geometric_progression#:~:text=In%20mathematics%2C%20a%20geometric%20progression,number%20called%20the%20common%20ratio.">geometric progression</a>, with \(\sigma_1\) being sufficiently small and \(\sigma_L\) comparable to the maximum pairwise distance between all training data points <d-cite key="song2020improved"></d-cite>. \(L\) is typically on the order of hundreds or thousands.</p> </li> <li> <p>Parameterize the score-based model \(\mathbf{s}_\theta(\mathbf{x}, i)\) with U-Net skip connections <d-cite key="song2019generative,ho2020denoising"></d-cite>.</p> </li> <li> <p>Apply exponential moving average on the weights of the score-based model when used at test time <d-cite key="song2020improved,ho2020denoising"></d-cite>.</p> </li> </ul> <p>With such best practices, we are able to generate high quality image samples with comparable quality to GANs on various datasets, such as below:</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/ncsnv2.jpg"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Samples from the NCSNv2<d-cite key="song2020improved"></d-cite> model. From left to right: FFHQ 256x256, LSUN bedroom 128x128, LSUN tower 128x128, LSUN church_outdoor 96x96, and CelebA 64x64.</figcaption> </div> <h2 id="score-based-generative-modeling-with-stochastic-differential-equations-sdes">Score-based generative modeling with stochastic differential equations (SDEs)</h2> <p>As we already discussed, adding multiple noise scales is critical to the success of score-based generative models. By generalizing the number of noise scales to infinity <d-cite key="song2021scorebased"></d-cite>, we obtain not only <strong>higher quality samples</strong>, but also, among others, <strong>exact log-likelihood computation</strong>, and <strong>controllable generation for inverse problem solving</strong>.</p> <p>In addition to this introduction, we have tutorials written in <a href="https://colab.research.google.com/">Google Colab</a> to provide a step-by-step guide for training a toy model on MNIST. We also have more advanced code repositories that provide full-fledged implementations for large scale applications.</p> <table> <thead> <tr> <th style="text-align: center">Link</th> <th style="text-align: left">Description</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><a href="https://colab.research.google.com/drive/1SeXMpILhkJPjXUaesvzEhc3Ke6Zl_zxJ?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td> <td style="text-align: left">Tutorial of score-based generative modeling with SDEs in JAX + FLAX</td> </tr> <tr> <td style="text-align: center"><a href="https://colab.research.google.com/drive/1dRR_0gNRmfLtPavX2APzUggBuXyjWW55?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td> <td style="text-align: left">Load our pretrained checkpoints and play with sampling, likelihood computation, and controllable synthesis (JAX + FLAX)</td> </tr> <tr> <td style="text-align: center"><a href="https://colab.research.google.com/drive/120kYYBOVa1i0TD85RjlEkFjaWDxSFUx3?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td> <td style="text-align: left">Tutorial of score-based generative modeling with SDEs in PyTorch</td> </tr> <tr> <td style="text-align: center"><a href="https://colab.research.google.com/drive/17lTrPLTt_0EDXa4hkbHmbAFQEkpRDZnh?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td> <td style="text-align: left">Load our pretrained checkpoints and play with sampling, likelihood computation, and controllable synthesis (PyTorch)</td> </tr> <tr> <td style="text-align: center"><a href="https://github.com/yang-song/score_sde">Code in JAX</a></td> <td style="text-align: left">Score SDE codebase in JAX + FLAX</td> </tr> <tr> <td style="text-align: center"><a href="https://github.com/yang-song/score_sde_pytorch">Code in PyTorch</a></td> <td style="text-align: left">Score SDE codebase in PyTorch</td> </tr> </tbody> </table> <h3 id="perturbing-data-with-an-sde">Perturbing data with an SDE</h3> <p>When the number of noise scales approaches infinity, we essentially perturb the data distribution with continuously growing levels of noise. In this case, the noise perturbation procedure is a continuous-time <a href="https://en.wikipedia.org/wiki/Stochastic_process#:~:text=A%20stochastic%20process%20is%20defined,measurable%20with%20respect%20to%20some">stochastic process</a>, as demonstrated below</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/perturb_vp.gif"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Perturbing data to noise with a continuous-time stochastic process.</figcaption> </div> <p>How can we represent a stochastic process in a concise way? Many stochastic processes (<a href="https://en.wikipedia.org/wiki/Diffusion_process">diffusion processes</a> in particular) are solutions of stochastic differential equations (SDEs). In general, an SDE possesses the following form:</p> \[\begin{align} \mathrm{d}\mathbf{x} = \mathbf{f}(\mathbf{x}, t) \mathrm{d}t + g(t) \mathrm{d} \mathbf{w},\label{sde} \end{align}\] <p>where \(\mathbf{f}(\cdot, t): \mathbb{R}^d \to \mathbb{R}^d\) is a vector-valued function called the drift coefficient, \(g(t)\in \mathbb{R}\) is a real-valued function called the diffusion coefficient, \(\mathbf{w}\) denotes a standard <a href="https://en.wikipedia.org/wiki/Brownian_motion">Brownian motion</a>, and \(\mathrm{d} \mathbf{w}\) can be viewed as infinitesimal white noise. The solution of a stochastic differential equation is a continuous collection of random variables \(\{ \mathbf{x}(t) \}_{t\in [0, T]}\). These random variables trace stochastic trajectories as the time index \(t\) grows from the start time \(0\) to the end time \(T\). Let \(p_t(\mathbf{x})\) denote the (marginal) probability density function of \(\mathbf{x}(t)\). Here \(t \in [0, T]\) is analogous to \(i = 1, 2, \cdots, L\) when we had a finite number of noise scales, and \(p_t(\mathbf{x})\) is analogous to \(p_{\sigma_i}(\mathbf{x})\). Clearly, \(p_0(\mathbf{x}) = p(\mathbf{x})\) is the data distribution since no perturbation is applied to data at \(t=0\). After perturbing \(p(\mathbf{x})\) with the stochastic process for a sufficiently long time \(T\), \(p_T(\mathbf{x})\) becomes close to a tractable noise distribution \(\pi(\mathbf{x})\), called a <strong>prior distribution</strong>. We note that \(p_T(\mathbf{x})\) is analogous to \(p_{\sigma_L}(\mathbf{x})\) in the case of finite noise scales, which corresponds to applying the largest noise perturbation \(\sigma_L\) to the data.</p> <p>The SDE in \eqref{sde} is <strong>hand designed</strong>, similarly to how we hand-designed \(\sigma_1 &lt; \sigma_2 &lt; \cdots &lt; \sigma_L\) in the case of finite noise scales. There are numerous ways to add noise perturbations, and the choice of SDEs is not unique. For example, the following SDE</p> \[\begin{align} \mathrm{d}\mathbf{x} = e^{t} \mathrm{d} \mathbf{w} \end{align}\] <p>perturbs data with a Gaussian noise of mean zero and exponentially growing variance, which is analogous to perturbing data with \(\mathcal{N}(0, \sigma_1^2 I), \mathcal{N}(0, \sigma_2^2 I), \cdots, \mathcal{N}(0, \sigma_L^2 I)\) when \(\sigma_1 &lt; \sigma_2 &lt; \cdots &lt; \sigma_L\) is a <a href="https://en.wikipedia.org/wiki/Geometric_progression#:~:text=In%20mathematics%2C%20a%20geometric%20progression,number%20called%20the%20common%20ratio.">geometric progression</a>. Therefore, the SDE should be viewed as part of the model, much like \(\{\sigma_1, \sigma_2, \cdots, \sigma_L\}\). In <d-cite key="song2021scorebased"></d-cite>, we provide three SDEs that generally work well for images: the Variance Exploding SDE (VE SDE), the Variance Preserving SDE (VP SDE), and the sub-VP SDE.</p> <h3 id="reversing-the-sde-for-sample-generation">Reversing the SDE for sample generation</h3> <p>Recall that with a finite number of noise scales, we can generate samples by reversing the perturbation process with <strong>annealed Langevin dynamics</strong>, i.e., sequentially sampling from each noise-perturbed distribution using Langevin dynamics. For infinite noise scales, we can analogously reverse the perturbation process for sample generation by using the reverse SDE.</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/denoise_vp.gif"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Generate data from noise by reversing the perturbation procedure.</figcaption> </div> <p>Importantly, any SDE has a corresponding reverse SDE <d-cite key="anderson1982reverse"></d-cite>, whose closed form is given by</p> \[\begin{equation} \mathrm{d}\mathbf{x} = [\mathbf{f}(\mathbf{x}, t) - g^2(t) \nabla_\mathbf{x} \log p_t(\mathbf{x})]\mathrm{d}t + g(t) \mathrm{d} \mathbf{w}.\label{rsde} \end{equation}\] <p>Here \(\mathrm{d} t\) represents a negative infinitesimal time step, since the SDE \eqref{rsde} needs to be solved backwards in time (from \(t=T\) to \(t = 0\)). In order to compute the reverse SDE, we need to estimate \(\nabla_\mathbf{x} \log p_t(\mathbf{x})\), which is exactly the <strong>score function</strong> of \(p_t(\mathbf{x})\).</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/sde_schematic.jpg"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Solving a reverse SDE yields a score-based generative model. Transforming data to a simple noise distribution can be accomplished with an SDE. It can be reversed to generate samples from noise if we know the score of the distribution at each intermediate time step. </figcaption> </div> <h3 id="estimating-the-reverse-sde-with-score-based-models-and-score-matching">Estimating the reverse SDE with score-based models and score matching</h3> <p>Solving the reverse SDE requires us to know the terminal distribution \(p_T(\mathbf{x})\), and the score function \(\nabla_\mathbf{x} \log p_t(\mathbf{x})\). By design, the former is close to the prior distribution \(\pi(\mathbf{x})\) which is fully tractable. In order to estimate \(\nabla_\mathbf{x} \log p_t(\mathbf{x})\), we train a <strong>Time-Dependent Score-Based Model</strong> \(\mathbf{s}_\theta(\mathbf{x}, t)\), such that \(\mathbf{s}_\theta(\mathbf{x}, t) \approx \nabla_\mathbf{x} \log p_t(\mathbf{x})\). This is analogous to the noise-conditional score-based model \(\mathbf{s}_\theta(\mathbf{x}, i)\) used for finite noise scales, trained such that \(\mathbf{s}_\theta(\mathbf{x}, i) \approx \nabla_\mathbf{x} \log p_{\sigma_i}(\mathbf{x})\).</p> <p>Our training objective for \(\mathbf{s}_\theta(\mathbf{x}, t)\) is a continuous weighted combination of Fisher divergences, given by</p> \[\begin{equation} \mathbb{E}_{t \in \mathcal{U}(0, T)}\mathbb{E}_{p_t(\mathbf{x})}[\lambda(t) \| \nabla_\mathbf{x} \log p_t(\mathbf{x}) - \mathbf{s}_\theta(\mathbf{x}, t) \|_2^2], \end{equation}\] <p>where \(\mathcal{U}(0, T)\) denotes a uniform distribution over the time interval \([0, T]\), and \(\lambda: \mathbb{R} \to \mathbb{R}_{&gt;0}\) is a positive weighting function. Typically we use \(\lambda(t) \propto 1/ \mathbb{E}[\| \nabla_{\mathbf{x}(t)} \log p(\mathbf{x}(t) \mid \mathbf{x}(0))\|_2^2]\) to balance the magnitude of different score matching losses across time.</p> <p>As before, our weighted combination of Fisher divergences can be efficiently optimized with score matching methods, such as denoising score matching <d-cite key="vincent2011connection"></d-cite> and sliced score matching <d-cite key="song2019sliced"> </d-cite>. Once our score-based model \(\mathbf{s}_\theta(\mathbf{x}, t)\) is trained to optimality, we can plug it into the expression of the reverse SDE in \eqref{rsde} to obtain an estimated reverse SDE.</p> \[\begin{equation} \mathrm{d}\mathbf{x} = [\mathbf{f}(\mathbf{x}, t) - g^2(t) \mathbf{s}_\theta(\mathbf{x}, t)]\mathrm{d}t + g(t) \mathrm{d} \mathbf{w}. \end{equation}\] <p>We can start with \(\mathbf{x}(T) \sim \pi\), and solve the above reverse SDE to obtain a sample \(\mathbf{x}(0)\). Let us denote the distribution of \(\mathbf{x}(0)\) obtained in such way as \(p_\theta\). When the score-based model \(\mathbf{s}_\theta(\mathbf{x}, t)\) is well-trained, we have \(p_\theta \approx p_0\), in which case \(\mathbf{x}(0)\) is an approximate sample from the data distribution \(p_0\).</p> <p>When \(\lambda(t) = g^2(t)\), we have an important connection between our weighted combination of Fisher divergences and the KL divergence from \(p_0\) to \(p_\theta\) under some regularity conditions <d-cite key="durkan2021maximum"></d-cite>:</p> \[\begin{multline} \operatorname{KL}(p_0(\mathbf{x})\|p_\theta(\mathbf{x})) \leq \frac{T}{2}\mathbb{E}_{t \in \mathcal{U}(0, T)}\mathbb{E}_{p_t(\mathbf{x})}[\lambda(t) \| \nabla_\mathbf{x} \log p_t(\mathbf{x}) - \mathbf{s}_\theta(\mathbf{x}, t) \|_2^2] \\+ \operatorname{KL}(p_T \mathrel\| \pi). \end{multline}\] <p>Due to this special connection to the KL divergence and the equivalence between minimizing KL divergences and maximizing likelihood for model training, we call \(\lambda(t) = g(t)^2\) the <strong>likelihood weighting function</strong>. Using this likelihood weighting function, we can train score-based generative models to achieve very high likelihoods, comparable or even superior to state-of-the-art autoregressive models<d-cite key="durkan2021maximum"></d-cite>.</p> <h3 id="how-to-solve-the-reverse-sde">How to solve the reverse SDE</h3> <p>By solving the estimated reverse SDE with numerical SDE solvers, we can simulate the reverse stochastic process for sample generation. Perhaps the simplest numerical SDE solver is the <a href="https://en.wikipedia.org/wiki/Euler%E2%80%93Maruyama_method">Euler-Maruyama method</a>. When applied to our estimated reverse SDE, it discretizes the SDE using finite time steps and small Gaussian noise. Specifically, it chooses a small negative time step \(\Delta t \approx 0\), initializes \(t \gets T\), and iterates the following procedure until \(t \approx 0\):</p> \[\begin{aligned} \Delta \mathbf{x} &amp;\gets [\mathbf{f}(\mathbf{x}, t) - g^2(t) \mathbf{s}_\theta(\mathbf{x}, t)]\Delta t + g(t) \sqrt{\vert \Delta t\vert }\mathbf{z}_t \\ \mathbf{x} &amp;\gets \mathbf{x} + \Delta \mathbf{x}\\ t &amp;\gets t + \Delta t, \end{aligned}\] <p>Here \(\mathbf{z}_t \sim \mathcal{N}(0, I)\). The Euler-Maruyama method is qualitatively similar to Langevin dynamics—both update \(\mathbf{x}\) by following score functions perturbed with Gaussian noise.</p> <p>Aside from the Euler-Maruyama method, other numerical SDE solvers can be directly employed to solve the reverse SDE for sample generation, including, for example, <a href="https://en.wikipedia.org/wiki/Milstein_method">Milstein method</a>, and <a href="https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_method_(SDE)">stochastic Runge-Kutta methods</a>. In <d-cite key="song2021scorebased"></d-cite>, we provided a reverse diffusion solver similar to Euler-Maruyama, but more tailored for solving reverse-time SDEs. More recently, authors in <d-cite key="jolicoeur2021gotta"></d-cite> introduced adaptive step-size SDE solvers that can generate samples faster with better quality.</p> <p>In addition, there are two special properties of our reverse SDE that allow for even more flexible sampling methods:</p> <ul> <li>We have an estimate of \(\nabla_\mathbf{x} \log p_t(\mathbf{x})\) via our time-dependent score-based model \(\mathbf{s}_\theta(\mathbf{x}, t)\).</li> <li>We only care about sampling from each marginal distribution \(p_t(\mathbf{x})\). Samples obtained at different time steps can have arbitrary correlations and do not have to form a particular trajectory sampled from the reverse SDE.</li> </ul> <p>As a consequence of these two properties, we can apply MCMC approaches to fine-tune the trajectories obtained from numerical SDE solvers. Specifically, we propose <strong>Predictor-Corrector samplers</strong>. The <strong>predictor</strong> can be any numerical SDE solver that predicts \(\mathbf{x}(t + \Delta t) \sim p_{t+\Delta t}(\mathbf{x})\) from an existing sample \(\mathbf{x}(t) \sim p_t(\mathbf{x})\). The <strong>corrector</strong> can be any MCMC procedure that solely relies on the score function, such as Langevin dynamics and Hamiltonian Monte Carlo.</p> <p>At each step of the Predictor-Corrector sampler, we first use the predictor to choose a proper step size \(\Delta t &lt; 0\), and then predict \(\mathbf{x}(t + \Delta t)\) based on the current sample \(\mathbf{x}(t)\). Next, we run several corrector steps to improve the sample \(\mathbf{x}(t + \Delta t)\) according to our score-based model \(\mathbf{s}_\theta(\mathbf{x}, t + \Delta t)\), so that \(\mathbf{x}(t + \Delta t)\) becomes a higher-quality sample from \(p_{t+\Delta t}(\mathbf{x})\).</p> <p>With Predictor-Corrector methods and better architectures of score-based models, we can achieve <strong>state-of-the-art</strong> sample quality on CIFAR-10 (measured in FID <d-cite key="FID"></d-cite> and Inception scores <d-cite key="salimans2016improved"></d-cite>), outperforming the best GAN model to date (StyleGAN2 + ADA <d-cite key="Karras2020ada"></d-cite>).</p> <table> <thead> <tr> <th style="text-align: center">Method</th> <th style="text-align: center">FID \(\downarrow\)</th> <th style="text-align: center">Inception score \(\uparrow\)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">StyleGAN2 + ADA <d-cite key="Karras2020ada"></d-cite></td> <td style="text-align: center">2.92</td> <td style="text-align: center">9.83</td> </tr> <tr> <td style="text-align: center">Ours <d-cite key="song2021scorebased"></d-cite></td> <td style="text-align: center"><strong>2.20</strong></td> <td style="text-align: center"><strong>9.89</strong></td> </tr> </tbody> </table> <p>The sampling methods are also scalable for extremely high dimensional data. For example, it can successfully generate high fidelity images of resolution \(1024\times 1024\).</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/ffhq_1024.jpeg"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> 1024 x 1024 samples from a score-based model trained on the FFHQ dataset. </figcaption> </div> <p>Some additional (uncurated) samples for other datasets (taken from this <a href="https://github.com/yang-song/score_sde">GitHub repo</a>):</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/bedroom.jpeg"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> 256 x 256 samples on LSUN bedroom. </figcaption> </div> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/celebahq_256.jpg"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> 256 x 256 samples on CelebA-HQ. </figcaption> </div> <h3 id="probability-flow-ode">Probability flow ODE</h3> <p>Despite capable of generating high-quality samples, samplers based on Langevin MCMC and SDE solvers do not provide a way to compute the exact log-likelihood of score-based generative models. Below, we introduce a sampler based on ordinary differential equations (ODEs) that allow for exact likelihood computation.</p> <p>In <d-cite key="song2021scorebased"></d-cite>, we show t is possible to convert any SDE into an ordinary differential equation (ODE) without changing its marginal distributions \(\{ p_t(\mathbf{x}) \}_{t \in [0, T]}\). Thus by solving this ODE, we can sample from the same distributions as the reverse SDE. The corresponding ODE of an SDE is named <strong>probability flow ODE</strong> <d-cite key="song2021scorebased"></d-cite>, given by</p> \[\begin{equation} \mathrm{d} \mathbf{x} = \bigg[\mathbf{f}(\mathbf{x}, t) - \frac{1}{2}g^2(t) \nabla_\mathbf{x} \log p_t(\mathbf{x})\bigg] \mathrm{d}t.\label{prob_ode} \end{equation}\] <p>The following figure depicts trajectories of both SDEs and probability flow ODEs. Although ODE trajectories are noticeably smoother than SDE trajectories, they convert the same data distribution to the same prior distribution and vice versa, sharing the same set of marginal distributions \(\{ p_t(\mathbf{x}) \}_{t \in [0, T]}\). In other words, trajectories obtained by solving the probability flow ODE have the same marginal distributions as the SDE trajectories.</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1" src="/assets/img/score/teaser.jpg"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> We can map data to a noise distribution (the prior) with an SDE, and reverse this SDE for generative modeling. We can also reverse the associated probability flow ODE, which yields a deterministic process that samples from the same distribution as the SDE. Both the reverse-time SDE and probability flow ODE can be obtained by estimating score functions. </figcaption> </div> <p>This probability flow ODE formulation has several unique advantages.</p> <p>When \(\nabla_\mathbf{x} \log p_t(\mathbf{x})\) is replaced by its approximation \(\mathbf{s}_\theta(\mathbf{x}, t)\), the probability flow ODE becomes a special case of a neural ODE<d-cite key="neural_ode"></d-cite>. In particular, it is an example of continuous normalizing flows<d-cite key="grathwohl2018scalable"></d-cite>, since the probability flow ODE converts a data distribution \(p_0(\mathbf{x})\) to a prior noise distribution \(p_T(\mathbf{x})\) (since it shares the same marginal distributions as the SDE) and is fully invertible.</p> <p>As such, the probability flow ODE inherits all properties of neural ODEs or continuous normalizing flows, including exact log-likelihood computation. Specifically, we can leverage the instantaneous change-of-variable formula (Theorem 1 in <d-cite key="neural_ode"></d-cite>, Equation (4) in <d-cite key="grathwohl2018scalable"></d-cite>) to compute the unknown data density \(p_0\) from the known prior density \(p_T\) with numerical ODE solvers.</p> <p>In fact, our model achieves the <strong>state-of-the-art</strong> log-likelihoods on uniformly dequantized <d-footnote>It is typical for normalizing flow models to convert discrete images to continuous ones by adding small uniform noise to them. </d-footnote> CIFAR-10 images <d-cite key="song2021scorebased"></d-cite>, <strong>even without maximum likelihood training</strong>.</p> <table> <thead> <tr> <th style="text-align: center">Method</th> <th style="text-align: center">Negative log-likelihood (bits/dim) \(\downarrow\)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">RealNVP</td> <td style="text-align: center">3.49</td> </tr> <tr> <td style="text-align: center">iResNet</td> <td style="text-align: center">3.45</td> </tr> <tr> <td style="text-align: center">Glow</td> <td style="text-align: center">3.35</td> </tr> <tr> <td style="text-align: center">FFJORD</td> <td style="text-align: center">3.40</td> </tr> <tr> <td style="text-align: center">Flow++</td> <td style="text-align: center">3.29</td> </tr> <tr> <td style="text-align: center">Ours</td> <td style="text-align: center"><strong>2.99</strong></td> </tr> </tbody> </table> <p>When training score-based models with the <strong>likelihood weighting</strong> we discussed before, and using <strong>variational dequantization</strong> to obtain likelihoods on discrete images, we can achieve comparable or even superior likelihood to the state-of-the-art autoregressive models (all without any data augmentation) <d-cite key="durkan2021maximum"></d-cite>.</p> <table> <thead> <tr> <th style="text-align: center">Method</th> <th style="text-align: center">Negative log-likelihood (bits/dim) \(\downarrow\) on CIFAR-10</th> <th style="text-align: center">Negative log-likelihood (bits/dim) \(\downarrow\) on ImageNet 32x32</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Sparse Transformer</td> <td style="text-align: center"><strong>2.80</strong></td> <td style="text-align: center">-</td> </tr> <tr> <td style="text-align: center">Image Transformer</td> <td style="text-align: center">2.90</td> <td style="text-align: center">3.77</td> </tr> <tr> <td style="text-align: center">Ours</td> <td style="text-align: center">2.83</td> <td style="text-align: center"><strong>3.76</strong></td> </tr> </tbody> </table> <h3 id="controllable-generation-for-inverse-problem-solving">Controllable generation for inverse problem solving</h3> <p>Score-based generative models are particularly suitable for solving inverse problems. At its core, inverse problems are same as Bayesian inference problems. Let \(\mathbf{x}\) and \(\mathbf{y}\) be two random variables, and suppose we know the forward process of generating \(\mathbf{y}\) from \(\mathbf{x}\), represented by the transition probability distribution \(p(\mathbf{y} \mid \mathbf{x})\). The inverse problem is to compute \(p(\mathbf{x} \mid \mathbf{y})\). From Bayes’ rule, we have \(p(\mathbf{x} \mid \mathbf{y}) = p(\mathbf{x}) p(\mathbf{y} \mid \mathbf{x}) / \int p(\mathbf{x}) p(\mathbf{y} \mid \mathbf{x}) \mathrm{d} \mathbf{x}\). This expression can be greatly simplified by taking gradients with respect to \(\mathbf{x}\) on both sides, leading to the following Bayes’ rule for score functions:</p> \[\begin{equation} \nabla_\mathbf{x} \log p(\mathbf{x} \mid \mathbf{y}) = \nabla_\mathbf{x} \log p(\mathbf{x}) + \nabla_\mathbf{x} \log p(\mathbf{y} \mid \mathbf{x}).\label{inverse_problem} \end{equation}\] <p>Through score matching, we can train a model to estimate the score function of the unconditional data distribution, i.e., \(\mathbf{s}_\theta(\mathbf{x}) \approx \nabla_\mathbf{x} \log p(\mathbf{x})\). This will allow us to easily compute the posterior score function \(\nabla_\mathbf{x} \log p(\mathbf{x} \mid \mathbf{y})\) from the known forward process \(p(\mathbf{y} \mid \mathbf{x})\) via equation \eqref{inverse_problem}, and sample from it with Langevin-type sampling <d-cite key="song2021scorebased"></d-cite>.</p> <p>A recent work from UT Austin <d-cite key="jalal2021robust"></d-cite> has demonstrated that score-based generative models can be applied to solving inverse problems in medical imaging, such as accelerating magnetic resonance imaging (MRI). Concurrently in <d-cite key="song2022solving"></d-cite>, we demonstrated superior performance of score-based generative models not only on accelerated MRI, but also sparse-view computed tomography (CT). We were able to achieve comparable or even better performance than supervised or unrolled deep learning approaches, while being more robust to different measurement processes at test time.</p> <p>Below we show some examples on solving inverse problems for computer vision.</p> <div class="l-body"> <img class="img-fluid rounded z-depth-1 center" src="/assets/img/score/class_cond.png" style="display: block; margin-left: auto; margin-right: auto;"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Class-conditional generation with an unconditional time-dependent score-based model, and a pre-trained noise-conditional image classifier on CIFAR-10. </figcaption> </div> <div class="l-body"> <img class="img-fluid rounded z-depth-1 center" src="/assets/img/score/inpainting.png" style="display: block; margin-left: auto; margin-right: auto;"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Image inpainting with a time-dependent score-based model trained on LSUN bedroom. The leftmost column is ground-truth. The second column shows masked images (y in our framework). The rest columns show different inpainted images, generated by solving the conditional reverse-time SDE. </figcaption> </div> <div class="l-body"> <img class="img-fluid rounded z-depth-1 center" src="/assets/img/score/colorization.png" style="display: block; margin-left: auto; margin-right: auto;"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> Image colorization with a time-dependent score-based model trained on LSUN church_outdoor and bedroom. The leftmost column is ground-truth. The second column shows gray-scale images (y in our framework). The rest columns show different colorizedimages, generated by solving the conditional reverse-time SDE. </figcaption> </div> <div class="l-body"> <img class="img-fluid rounded z-depth-1 center" src="/assets/img/score/lincoln.png" style="display: block; margin-left: auto; margin-right: auto;"/> <figcaption style="text-align: center; margin-top: 10px; margin-bottom: 10px;"> We can even colorize gray-scale portrays of famous people in history (Abraham Lincoln) with a time-dependent score-based model trained on FFHQ. The image resolution is 1024 x 1024.</figcaption> </div> <h2 id="connection-to-diffusion-models-and-others">Connection to diffusion models and others</h2> <p>I started working on score-based generative modeling since 2019, when I was trying hard to make score matching scalable for training deep energy-based models on high-dimensional datasets. My first attempt at this led to the method sliced score matching<d-cite key="song2019sliced"></d-cite>. Despite the scalability of sliced score matching for training energy-based models, I found to my surprise that Langevin sampling from those models fails to produce reasonable samples even on the MNIST dataset. I started investigating this issue and discovered three crucial improvements that can lead to extremely good samples: (1) perturbing data with multiple scales of noise, and training score-based models for each noise scale; (2) using a U-Net architecture (we used RefineNet since it is a modern version of U-Nets) for the score-based model; (3) applying Langevin MCMC to each noise scale and chaining them together. With those methods, I was able to obtain the state-of-the-art Inception Score on CIFAR-10 in <d-cite key="song2019generative"></d-cite> (even better than the best GANs!), and generate high-fidelity image samples of resolution up to $256\times 256$ in <d-cite key="song2020improved"></d-cite>.</p> <p>The idea of perturbing data with multiple scales of noise is by no means unique to score-based generative models though. It has been previously used in, for example, <a href="https://en.wikipedia.org/wiki/Simulated_annealing">simulated annealing</a>, annealed importance sampling<d-cite key="neal2001annealed"></d-cite>, diffusion probabilistic models<d-cite key="sohl2015deep"></d-cite>, infusion training<d-cite key="bordes2017learning"></d-cite>, and variational walkback<d-cite key="goyal2017variational"></d-cite> for generative stochastic networks<d-cite key="alain2016gsns"></d-cite>. Out of all these works, diffusion probabilistic modeling is perhaps the closest to score-based generative modeling. Diffusion probabilistic models are hierachical latent variable models first proposed by <a href="http://www.sohldickstein.com/">Jascha</a> and his colleagues <d-cite key="sohl2015deep"></d-cite> in 2015, which generate samples by learning a variational decoder to reverse a discrete diffusion process that perturbs data to noise. Without awareness of this work, score-based generative modeling was proposed and motivated independently from a very different perspective. Despite both perturbing data with multiple scales of noise, the connection between score-based generative modeling and diffusion probabilistic modeling seemed superficial at that time, since the former is trained by score matching and sampled by Langevin dynamics, while the latter is trained by the evidence lower bound (ELBO) and sampled with a learned decoder.</p> <p>In 2020, <a href="http://www.jonathanho.me/">Jonathan Ho</a> and colleagues <d-cite key="ho2020denoising"></d-cite> significantly improved the empirical performance of diffusion probabilistic models and first unveiled a deeper connection to score-based generative modeling. They showed that the ELBO used for training diffusion probabilistic models is essentially equivalent to the weighted combination of score matching objectives used in score-based generative modeling. Moreover, by parameterizing the decoder as a sequence of score-based models with a U-Net architecture, they demonstrated for the first time that diffusion probabilistic models can also generate high quality image samples comparable or superior to GANs.</p> <p>Inspired by their work, we further investigated the relationship between diffusion models and score-based generative models in an ICLR 2021 paper <d-cite key="song2021scorebased"></d-cite>. We found that the sampling method of diffusion probabilistic models can be integrated with annealed Langevin dynamics of score-based models to create a unified and more powerful sampler (the Predictor-Corrector sampler). By generalizing the number of noise scales to infinity, we further proved that score-based generative models and diffusion probabilistic models can both be viewed as discretizations to stochastic differential equations determined by score functions. This work bridges both score-based generative modeling and diffusion probabilistic modeling into a unified framework.</p> <p>Collectively, these latest developments seem to indicate that both score-based generative modeling with multiple noise perturbations and diffusion probabilistic models are different perspectives of the same model family, much like how <a href="https://en.wikipedia.org/wiki/Wave_mechanics">wave mechanics</a> and <a href="https://en.wikipedia.org/wiki/Matrix_mechanics">matrix mechanics</a> are equivalent formulations of quantum mechanics in the history of physics<d-footnote>Goes without saying that the significance of score-based generative models/diffusion probabilistic models is in no way comparable to quantum mechanics.</d-footnote>. The perspective of score matching and score-based models allows one to calculate log-likelihoods exactly, solve inverse problems naturally, and is directly connected to energy-based models, Schrödinger bridges and optimal transport<d-cite key="de2021diffusion"></d-cite>. The perspective of diffusion models is naturally connected to VAEs, lossy compression, and can be directly incorporated with variational probabilistic inference. This blog post focuses on the first perspective, but I highly recommend interested readers to learn about the alternative perspective of diffusion models as well (see <a href="https://lilianweng.github.io/lil-log/2021/07/11/diffusion-models.html">a great blog by Lilian Weng</a>).</p> <p>Many recent works on score-based generative models or diffusion probabilistic models have been deeply influenced by knowledge from both sides of research (see a <a href="https://scorebasedgenerativemodeling.github.io/">website</a> curated by researchers at the University of Oxford). Despite this deep connection between score-based generative models and diffusion models, it is hard to come up with an umbrella term for the model family that they both belong to. Some colleagues in DeepMind propose to call them “Generative Diffusion Processes”. It remains to be seen if this will be adopted by the community in the future.</p> <h2 id="concluding-remarks">Concluding remarks</h2> <p>This blog post gives a detailed introduction to score-based generative models. We demonstrate that this new paradigm of generative modeling is able to produce high quality samples, compute exact log-likelihoods, and perform controllable generation for inverse problem solving. It is a compilation of several papers we published in the past few years. Please visit them if you are interested in more details:</p> <ul> <li> <p><a href="https://arxiv.org/abs/1905.07088">Yang Song*, Sahaj Garg*, Jiaxin Shi, and Stefano Ermon. <em>Sliced Score Matching: A Scalable Approach to Density and Score Estimation</em>. UAI 2019 (Oral)</a></p> </li> <li> <p><a href="https://arxiv.org/abs/1907.05600">Yang Song, and Stefano Ermon. <em>Generative Modeling by Estimating Gradients of the Data Distribution</em>. NeurIPS 2019 (Oral)</a></p> </li> <li> <p><a href="https://arxiv.org/abs/2006.09011">Yang Song, and Stefano Ermon. <em>Improved Techniques for Training Score-Based Generative Models</em>. NeurIPS 2020</a></p> </li> <li> <p><a href="https://arxiv.org/abs/2011.13456">Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. <em>Score-Based Generative Modeling through Stochastic Differential Equations</em>. ICLR 2021 (Outstanding Paper Award)</a></p> </li> <li> <p><a href="https://arxiv.org/abs/2101.09258">Yang Song*, Conor Durkan*, Iain Murray, and Stefano Ermon. <em>Maximum Likelihood Training of Score-Based Diffusion Models</em>. NeurIPS 2021 (Spotlight)</a></p> </li> <li> <p><a href="https://arxiv.org/abs/2111.08005">Yang Song*, Liyue Shen*, Lei Xing, and Stefano Ermon. <em>Solving Inverse Problems in Medical Imaging with Score-Based Generative Models</em>. ICLR 2022</a></p> </li> </ul> <p>For a list of works that have been influenced by score-based generative modeling, researchers at the University of Oxford have built a very useful (but necessarily incomplete) website: <a href="https://scorebasedgenerativemodeling.github.io/">https://scorebasedgenerativemodeling.github.io/</a>.</p> <p>There are two major challenges of score-based generative models. First, the sampling speed is slow since it involves a large number of Langevin-type iterations. Second, it is inconvenient to work with discrete data distributions since scores are only defined on continuous distributions.</p> <p>The first challenge can be partially solved by using numerical ODE solvers for the probability flow ODE with lower precision (a similar method, denoising diffusion implicit modeling, has been proposed in <d-cite key="song2021denoising"></d-cite>). It is also possible to learn a direct mapping from the latent space of probability flow ODEs to the image space, as shown in <d-cite key="luhman2021knowledge"></d-cite>. However, all such methods to date result in worse sample quality.</p> <p>The second challenge can be addressed by learning an autoencoder on discrete data and performing score-based generative modeling on its continuous latent space <d-cite key="mittal2021symbolic,vahdat2021score"></d-cite>. Jascha’s original work on diffusion models <d-cite key="sohl2015deep"></d-cite> also provides a discrete diffusion process for discrete data distributions, but its potential for large scale applications remains yet to be proven.</p> <p>It is my conviction that these challenges will soon be solved with the joint efforts of the research community, and score-based generative models/ diffusion-based models will become one of the most useful tools for data generation, density estimation, inverse problem solving, and many other downstream tasks in machine learning.</p>]]></content><author><name>Yang Song</name></author><summary type="html"><![CDATA[This blog post focuses on a promising new direction for generative modeling. We can learn score functions (gradients of log probability density functions) on a large number of noise-perturbed data distributions, then generate samples with Langevin-type sampling. The resulting generative models, often called score-based generative models, has several important advantages over existing model families: GAN-level sample quality without adversarial training, flexible model architectures, exact log-likelihood computation, and inverse problem solving without re-training models. In this blog post, we will show you in more detail the intuition, basic concepts, and potential applications of score-based generative models.]]></summary></entry><entry><title type="html">a post with github metadata</title><link href="https://yang-song.net/blog/2020/github-metadata/" rel="alternate" type="text/html" title="a post with github metadata"/><published>2020-09-28T21:01:00+00:00</published><updated>2020-09-28T21:01:00+00:00</updated><id>https://yang-song.net/blog/2020/github-metadata</id><content type="html" xml:base="https://yang-song.net/blog/2020/github-metadata/"><![CDATA[<p>A sample blog page that demonstrates the accessing of github meta data.</p> <h2 id="what-does-github-metadata-do">What does Github-MetaData do?</h2> <ul> <li>Propagates the site.github namespace with repository metadata</li> <li>Setting site variables : <ul> <li>site.title</li> <li>site.description</li> <li>site.url</li> <li>site.baseurl</li> </ul> </li> <li>Accessing the metadata - duh.</li> <li>Generating edittable links.</li> </ul> <h2 id="additional-reading">Additional Reading</h2> <ul> <li>If you’re recieving incorrect/missing data, you may need to perform a Github API<a href="https://github.com/jekyll/github-metadata/blob/master/docs/authentication.md"> authentication</a>.</li> <li>Go through this <a href="https://jekyll.github.io/github-metadata/">README</a> for more details on the topic.</li> <li><a href="https://github.com/jekyll/github-metadata/blob/master/docs/site.github.md">This page</a> highlights all the feilds you can access with github-metadata. <br/></li> </ul> <h2 id="example-metadata">Example MetaData</h2> <ul> <li>Host Name :</li> <li>URL :</li> <li>BaseURL :</li> <li>Archived :</li> <li>Contributors :</li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="external-services"/><summary type="html"><![CDATA[a quick run down on accessing github metadata.]]></summary></entry><entry><title type="html">Sliced Score Matching: A Scalable Approach to Density and Score Estimation</title><link href="https://yang-song.net/blog/2019/ssm/" rel="alternate" type="text/html" title="Sliced Score Matching: A Scalable Approach to Density and Score Estimation"/><published>2019-07-20T00:00:00+00:00</published><updated>2019-07-20T00:00:00+00:00</updated><id>https://yang-song.net/blog/2019/ssm</id><content type="html" xml:base="https://yang-song.net/blog/2019/ssm/"><![CDATA[<h3 id="unnormalized-probability-models">Unnormalized probability models</h3> <p>Let \(f_\theta(\mathbf{x})\) be a real-valued function on \(\mathbf{x} \in \mathbb{R}^D\). We can define a probability model from \(f_\theta(\mathbf{x})\), using the following formula</p> \[p_\theta(\mathbf{x}) = \dfrac{e^{-f_\theta(\mathbf{x})}}{Z_\theta}.\] <p>Here we assume that \(Z_\theta = \int e^{-f_\theta(\mathbf{x})} \text{d}\mathbf{x}\) exists and call it the <em>partition function</em>. Typically, it is intractable to evaluate the partition function, and \(Z_\theta\) is an unknown quantity. The probability model \(p_\theta(\mathbf{x})\) is called an <em>unnormalized probability model</em>, because the normalizing constant \(Z_\theta\) is unknown.</p> <h3 id="learning-unnormalized-models-with-score-matching">Learning unnormalized models with score matching</h3> <p>The de facto standard for learning a probability model is <em>maximum likelihood (MLE)</em>. However, it is not suitable for learning unnormalized probability models, because the likelihood function</p> \[\log p_\theta(\mathbf{x}) = -f_\theta(\mathbf{x}) - \log Z_\theta\] <p>depends on the intractable partition function \(Z_\theta\).</p> <p>Score matching <d-cite key="hyvarinen2005estimation"></d-cite> bypasses the intractable \(Z_\theta\) by only considering the <strong>scores</strong>, which is defined as <strong>the gradient of the log-density w.r.t. the random variable</strong>. For example, the score of \(p_\theta(\mathbf{x})\) is</p> \[\nabla_{\mathbf{x}} \log p_\theta(\mathbf{x}) = -\nabla_{\mathbf{x}} f_\theta(\mathbf{x}).\] <p>Note that the score of an unnormalized probability model \(\nabla_{\mathbf{x}} \log p_\theta(\mathbf{x})\) does not depend on the intractable partition function \(Z_\theta\). Instead of using likelihood as the objective, score matching is based on the following objective</p> \[\frac{1}{2}\mathbb{E}_{p_\text{data}}[\|\nabla_\mathbf{x} \log p_\text{data}(\mathbf{x}) - \nabla_\mathbf{x} \log p_\theta(\mathbf{x}) \|_2^2],\] <p>where \(p_\text{data}(\mathbf{x})\) is the data distribution. The above objective is widely known as the <em>Fisher divergence</em>. Since it only involves \(\nabla_\mathbf{x} \log p_\theta(\mathbf{x})\) and does not require \(Z_\theta\), it is ideal for training unnormalized models.</p> <p>However, Fisher divergence is not directly computable, because the score of the data distribution \(\nabla_\mathbf{x} \log p_\text{data}(\mathbf{x})\) is unknown. Score matching eliminates the data score using integration by parts. To simplify our discussion, we consider the Fisher divergence between distributions of 1-D random variables. We have</p> \[\begin{aligned} &amp;\frac{1}{2}\mathbb{E}_{p_\text{data}}[(\nabla_x \log p_\text{data}(x) - \nabla_x \log p_\theta(x))^2] \\ =&amp; \frac{1}{2} \int p_\text{data}(x) (\nabla_x \log p_\text{data}(x) - \nabla_x \log p_\theta(x))^2 \text{d}x\\ =&amp; \underbrace{\frac{1}{2} \int p_\text{data}(x) (\nabla_x \log p_\text{data}(x))^2 \text{d}x}_{\text{const}} + \frac{1}{2} \int p_\text{data}(x) (\nabla_x \log p_\theta(x))^2 \text{d} x \\ &amp;- \int p_\text{data}(x) \nabla_x \log p_\theta(x) \nabla_x \log p_\text{data}(x)\text{d}x. \end{aligned}\] <p>By integration by parts, we have</p> \[\begin{aligned} &amp;- \int p_\text{data}(x) \nabla_x \log p_\theta(x) \nabla_x \log p_\text{data}(x) \text{d}x\\ =&amp; - \int \nabla_x \log p_\theta(x) \nabla_x p_\text{data}(x)\text{d} x\\ =&amp; - p_\text{data}(x) \nabla_x \log p_\theta(x)\bigg|_{-\infty}^{\infty} + \int p_\text{data}(x) \nabla_x^2 \log p_\theta(x) \text{d} x\\ \stackrel{(i)}{=}&amp; \mathbb{E}_{p_\text{data}}[\nabla_x^2 \log p_\theta(x)], \end{aligned}\] <p>where \((i)\) holds if we assume \(p_\text{data}(x) \rightarrow 0\) when \(\vert x \vert \rightarrow \infty\). Now, substituting the results of integration by parts into the 1-D Fisher divergence, we obtain</p> \[\begin{aligned} &amp;\frac{1}{2}\mathbb{E}_{p_\text{data}}[(\nabla_x \log p_\text{data}(x) - \nabla_x \log p_\theta(x))^2]\\ =&amp; \mathbb{E}_{p_\text{data}}[\nabla_x^2 \log p_\theta(x)] + \frac{1}{2} \mathbb{E}_{p_\text{data}}[(\nabla_x \log p_\theta(x))^2] + \text{const}. \end{aligned}\] <p>Therefore, the equivalent form of 1-D Fisher divergence does not involve \(\nabla_x \log p_\text{data}(x)\).</p> <p>Generalizing the integration by parts argument to muti-dimensional data, we have the following objective equivalent to Fisher divergence (see <d-cite key=" hyvarinen2005estimation"></d-cite> for details of proof)</p> \[\mathbb{E}_{p_\text{data}}\bigg[\operatorname{tr}( \nabla_{\mathbf{x}}^2 \log p_\theta(\mathbf{x})) + \frac{1}{2} \| \nabla_\mathbf{x} \log p_\theta(\mathbf{x})\|_2^2 \bigg] + \text{const},\] <p>where \(\nabla_\mathbf{x}^2\) denotes the Hessian with respect to \(\mathbf{x}\). This objective is known as the score matching objective. Since it only involves functions of \(\nabla_\mathbf{x} \log p_\theta(\mathbf{x})\), it does not depend on the intractable partition function and therefore is ideal for learning unnormalized probability models.</p> <h3 id="sliced-score-matching">Sliced score matching</h3> <p>So far, we know that score matching can be used to learn unnormalized models. We are particularly interested in learning deep energy-based models, a special kind of unnormalized models where \(f_\theta(\mathbf{x})\) is parameterized by a deep neural network. In order to use score matching for learning deep energy-based models, we have to compute \(\| \nabla_\mathbf{x} \log p_\theta(\mathbf{x})\|_2^2 = \| \nabla_\mathbf{x} f_\theta(\mathbf{x})\|^2_2\) and \(\operatorname{tr}( \nabla_{\mathbf{x}}^2 \log p_\theta(\mathbf{x})) = -\operatorname{tr}( \nabla_{\mathbf{x}}^2 f_\theta(\mathbf{x}))\). The former can be computed by one simple backpropagation of \(f_\theta(\mathbf{x})\). The later, however, requires much more number of backpropagations to compute. As studied in <d-cite key="martens2012estimating"></d-cite>, computing \(\operatorname{tr}( \nabla_{\mathbf{x}}^2 f_\theta(\mathbf{x}))\) requires a number of backpropagation that is proportional to the data dimension \(D\). Therefore, score matching is not scalable when learning deep energy-based models on high-dimensional data.</p> <p>We propose <strong>sliced score matching</strong> to greatly scale up the computation of score matching. The motivating idea is that one dimensional data distribution is much easier to estimate for score matching. We propose to project the scores onto random directions, such that the vector fields of scores of the data and model distribution become scalar fields. We then compare the scalar fields to determine how far the model distribution is from the data distribution. It is clear to see that the two vector fields are equivalent if and only if their scalar fields corresponding to projections onto all directions are the same.</p> <p>Mathematically, we denote \(\mathbf{v}\) as the random projection direction, and \(p_\mathbf{v}\) as its distribution. The random projected version of Fisher divergence is</p> \[\frac{1}{2}\mathbb{E}_{p_\text{data}}[(\mathbf{v}^\intercal \nabla_\mathbf{x} \log p_\text{data}(\mathbf{x}) - \mathbf{v}^\intercal \nabla_\mathbf{x} \log p_\theta(\mathbf{x}) )^2],\] <p>which we name as the <em>sliced Fisher divergence</em>. Unfortunately, sliced Fisher divergence has the same problem as Fisher divergence, due to the unknown data score function \(\nabla_\mathbf{x} \log p_\text{data}(\mathbf{x})\). We therefore play the same trick of integration by parts, as done in score matching, to obtain the following tractable alternative form</p> \[\mathbb{E}_{p_\text{data}}\bigg[\mathbf{v}^\intercal \nabla_{\mathbf{x}}^2 \log p_\theta(\mathbf{x})\mathbf{v} + \frac{1}{2} (\mathbf{v}^\intercal\nabla_\mathbf{x} \log p_\theta(\mathbf{x}))^2 \bigg] + \text{const},\] <p>which is our sliced score matching objective. Again, \(\mathbf{v}^\intercal\nabla_\mathbf{x} \log p_\theta(\mathbf{x})\) can be computed by one backpropagation for deep energy-based models. The first term of the objective again involves Hessian, but it is in the form of Hessian-vector products, which can be computed within \(O(1)\) backpropagations. Therefore, the computation of sliced score matching does not depend on the dimension of data, and is much more scalable for training deep energy-based models on high dimensional datasets.</p> <h3 id="theoretical-guarantees-of-learning-with-sliced-score-matching">Theoretical guarantees of learning with sliced score matching</h3> <p>Suppose our data points \(\{ \mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_N\}\) are i.i.d. samples from the data distribution \(p_\text{data}\). For each data point \(\mathbf{x}_i\), we randomly draw \(M\) random projection directions \(\{ \mathbf{v}_{i1}, \mathbf{v}_{i2}, \cdots, \mathbf{v}_{iM}\} \sim p_\mathbf{v}\). The sliced score matching objective can be estimated with empirical averages, giving rise to the following finite-sample estimator:</p> \[\frac{1}{NM} \sum_{i=1}^N \sum_{j=1}^M \bigg[\mathbf{v}_{ij}^\intercal \nabla_{\mathbf{x}}^2 \log p_\theta(\mathbf{x}_i)\mathbf{v}_{ij} + \frac{1}{2} (\mathbf{v}_{ij}^\intercal\nabla_\mathbf{x} \log p_\theta(\mathbf{x}_i))^2 \bigg]\] <p>We denote \(\hat{\theta}_{N,M}\) as the minimizer of the above empirical estimator, and let \(\theta^*\) denote the true parameter corresponding to the data distribution such that \(p_{\theta^*} = p_\text{data}\). We can prove that under some regularity conditions, \(\hat{\theta}_{N,M}\) is consistent and asymptotically normal. More rigorously, for any \(M \in \mathbb{N}^+\), when \(N \rightarrow \infty\), we have</p> \[\hat{\theta}_{N,M} \stackrel{p}{\rightarrow} \theta^*\] <p>and</p> \[\sqrt{N}(\hat{\theta}_{N,M} - \theta^*) \stackrel{d}{\rightarrow} \mathcal{N}(0,\Sigma)\] <p>where \(\Sigma\) is some covariance matrix.</p> <h3 id="experiments">Experiments</h3> <p>Sliced score mathing has numerous applications. For example,</p> <ul> <li><strong>Density estimation</strong>. It can be used to scalably train deep energy-based models on high dimensional data.</li> <li><strong>Score estimation</strong>. The sliced score matching objective can be used to estimate the score of any distribution from which samples can be efficiently obtained, which we call score estimation. It can be used to learn Variational Auto-Encoders (VAE) <d-cite key="kingma2013auto"></d-cite> where the encoder is an implicit distribution. It can also be used to learn Wasserstein Auto-Encoders (WAEs) <d-cite key="tolstikhin2017wasserstein"></d-cite></li> <li><strong>Score-based generative modeling</strong> <d-cite key="song2019generative"></d-cite>. It can be used to estimate scores of the data distribution and produce samples directly with Langevin dynamics.</li> </ul>]]></content><author><name>Yang Song</name></author><summary type="html"><![CDATA[An overview for our UAI 2019 paper on Sliced Score Matching. We show how to use random projections to scale up score matching—a classic method to learn unnormalized probabilisic models—to high-dimensional data. Theoretically, sliced score matching produces a consistent and asymptotic normal estimator under some regularity conditions. We apply sliced score matching to training deep energy-based models, learning VAEs with implicit encoders and training Wasserstein Auto-Encoders (WAEs).]]></summary></entry><entry><title type="html">Accelerating Natural Gradient with Higher-Order Invariance</title><link href="https://yang-song.net/blog/2018/geo/" rel="alternate" type="text/html" title="Accelerating Natural Gradient with Higher-Order Invariance"/><published>2018-09-04T00:00:00+00:00</published><updated>2018-09-04T00:00:00+00:00</updated><id>https://yang-song.net/blog/2018/geo</id><content type="html" xml:base="https://yang-song.net/blog/2018/geo/"><![CDATA[<p>Probabilistic models are important for inference and prediction in many areas of machine learning. In this post, we focus our discussion on parametric probabilistic models, e.g., models that can be determined by specifying its parameters. Such a model may have several different <a href="https://en.wikipedia.org/wiki/Parametrization"><strong>parameterizations</strong></a>. For illustrative purposes, let’s consider a simple linear regression model that captures the joint distribution of input \(x\) and label \(y\):</p> \[p_\theta(x, y) = p(x) \mathcal{N}(y \mid \theta x + b, \sigma^2), \theta \in \mathbb{R},\] <p>where \(\theta\) is the parameter, and \(b, \sigma\) are two fixed constants. By substituing \(\theta\) with \(2\mu\), a different parameterization can be created</p> \[p_\mu(x, y) = p(x) \mathcal{N}(y \mid 2\mu x + b, \sigma^2), \mu \in \mathbb{R}.\] <p>Note that even if \(p_\theta(x,y)\) and \(p_\mu(x, y)\) have different analytical forms, they actually represent the same model family. In particular, \(p_{\theta=a}(x,y)\equiv p_{\mu=a/2}(x,y)\) for any \(a \in \mathbb{R}\).</p> <p>Given a loss function \(L(p(x, y))\), the goal of an optimization algorithm is to find the model \(p^\ast(x, y)\) such that \(L(p^\ast(x, y))\) is as small as possible, in the form of finding the optimal parameter \(\theta^\ast\) or \(\mu^\ast\) under some parameterization. <strong>Although re-parameterization does not change the model family, the performance of optimization algorithms usually differs under different parameterizations.</strong> In the following section, we use the linear regression model as a running example to elaborate on why this can happen.</p> <h3 id="why-optimization-can-depend-on-parameterization">Why optimization can depend on parameterization</h3> <p>Returning to the example of linear regression, a typical loss function is the expected negative log-likelihood, which can be formally written as</p> \[\begin{aligned} L(p(x,y)) &amp;= - \mathbb{E}_x [\log p(y\mid x)]\\ &amp;= \mathbb{E}_x \left[\frac{1}{2\sigma^2} (\theta x + b - y)^2 + \log \sqrt{2\pi} \sigma \right] \quad \text{(for $p_\theta(x,y)$)}\\ &amp;= \mathbb{E}_x \left[\frac{1}{2\sigma^2} (2\mu x + b - y)^2 + \log \sqrt{2\pi} \sigma \right] \quad \text{(for $p_\mu(x,y)$)} \end{aligned}\] <p>Consider using <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a> to minimize the loss function:</p> <ul> <li>For \(p_\theta(x,y)\), the update rule is \(\theta_{t+1} = \theta_t - \alpha \nabla_{\theta_t} L(p_{\theta_t}(x,y))\), where \(\alpha\) is the learning rate and</li> </ul> \[\nabla_{\theta_t}L(p_{\theta_t}(x,y)) = \mathbb{E}_x\left[ \frac{x}{\sigma^2}(\theta_t x + b - y) \right]\] <ul> <li>For \(p_\mu(x,y)\), the update rule is \(\mu_{t+1} = \mu_{t} - \alpha \nabla_{\mu_t} L(p_{\mu_t}(x,y))\), where \(\alpha\) is the learning rate and gradient becomes</li> </ul> \[\nabla_{\mu_t}L(p_{\mu_t}(x,y)) = \mathbb{E}_x\left[ \frac{2x}{\sigma^2}(2\mu_t x + b - y) \right]\] <p>Let’s assume \(\theta_t = 2\mu_t = a\), i.e., at \(t\)-th optimization step, \(p_{\theta_t = a}(x,y)\) represents the same probabilistic model as \(p_{\mu_t = a/2}(x,y)\). However, because of this, the gradient has different scales:</p> \[\nabla_{\theta_t} L(p_{\theta_t}(x,y))\vert_{\theta_t = a} = \frac{1}{2} \nabla_{\mu_t} L(p_{\mu_t}(x,y))\vert_{\mu_t = a/2}\] <p>and therefore</p> \[\begin{aligned} \theta_{t+1} &amp;= \theta_{t} - \alpha \nabla_{\theta_t} L(p_{\theta_t}(x,y))\\ &amp;= 2 \mu_t - \frac{\alpha}{2} \nabla_{\mu_t} L(p_{\mu_t}(x,y))\\ &amp;\neq 2\mu_t - 2\alpha \nabla_{\mu_t} L(p_{\mu_t}(x,y))\\ &amp;= 2\mu_{t+1}. \end{aligned}\] <p>Hence the \((t+1)\)-th optimization step will result in different probabilistic models \(p_{\theta_{t+1}}(x,y) \not \equiv p_{\mu_{t+1}}(x,y)\). Specifically, one step of gradient descent will result in <strong>different models</strong> depending on <strong>which parameterization is used</strong>. This dependence on model parameterization can be undesirable, as it requires carefully choosing the parameterization to avoid hindering optimization.</p> <p>For example, many tweaks are used to parameterize neural networks in such a way that they become more amenable to commonly used first-order optimization methods. We can view normalization methods of network activations, such as Batch Normalization <d-cite key="ioffe2015batch"></d-cite>, Layer Normalization <d-cite key="ba2016layer"></d-cite>, and Weight Normalization <d-cite key="salimans2016weight"></d-cite>, as special parameterizations of the network to facilitate first-order gradient descent methods.</p> <p>Luckily, this dependence on model parameterization is not universally true for every optimization method. For example, the <a href="https://en.wikipedia.org/wiki/Newton%27s_method">Newton-Raphson method</a> is invariant to <a href="https://en.wikipedia.org/wiki/Affine_transformation">affine transformations</a> of model parameters. Therefore, for the linear regression case discussed above, the Newton-Raphson method will ensure that \(p_{\theta_{t+1}}(x,y) \equiv p_{\mu_{t+1}}(x,y)\) as long as \(p_{\theta_t}(x,y) \equiv p_{\mu_t}(x,y)\), because the new parameter \(\mu = \frac{1}{2}\theta\) is an affine transformation of \(\theta\).</p> <p>Following this thread, we next discuss the Natural Gradient Method <d-cite key="amari1998natural"></d-cite>—an optimization method that uses a similar preconditioning matrix to that of the Newton-Raphson method, but is said to be <strong>invariant to arbitrary differentiable transformations of model parameters</strong> when the learning rate is <strong>small</strong> enough.</p> <h3 id="natural-gradient-method">Natural gradient method</h3> <p>From the example of applying gradient descent to linear regression, we can easily see that switching between model parameterizations can cause the gradients and parameters to change in different ways. So how can we ensure that when the parameterization changes, the gradient will also change in a clever way to compensate for its effect on these optimization algorithms?</p> <p>In 1998, inspired by research on <a href="https://en.wikipedia.org/wiki/Information_geometry">information geometry</a>, <a href="https://en.wikipedia.org/wiki/Shun'ichi_Amari">Shun’ichi Amari</a> proposed to use natural gradient <d-cite key="amari1998natural"></d-cite> to solve this problem. A full understanding of natural gradient requires some basic concepts from <a href="https://en.wikipedia.org/wiki/Riemannian_geometry">Riemannian geometry</a>. Although it is not required for understanding the rest of the post, we encourage interested readers to review the following introduction to Riemannian geometry.</p> <hr/> <details> <summary> <h5 style="text-align: center" id="an-introduction-to-riemannian-geometry">An Introduction to Riemannian Geometry</h5> <p style="text-align: center">(click to expand or collapse)</p> </summary> <p>The purpose of this text is to give readers the minimal background to understand the motivations behind natural gradient, invariance and our proposed improvements in the later part of this blog. For a more rigorous treatment on this topic, please refer to textbooks <d-cite key="amari2007methods"></d-cite> and <d-cite key="petersen2006riemannian"></d-cite>. Good theory starts with beautiful notation. We will first introduce <a href="https://en.wikipedia.org/wiki/Einstein_notation">Einstein summation convention</a>, a great method to eschew boilerplates in differential geometry formulas.</p> <h5 id="einstein-summation-convention">Einstein summation convention</h5> <p>The convention states that</p> <blockquote> <p>When any index variable appears twice in a term, once as a superscript and once as a subscript, it indicates summation of the term over all possible values of the index variable.</p> </blockquote> <p>For example, \(a^\mu b_\mu := \sum_{\mu=1}^n a^\mu b_\mu\) when index variable \(\mu \in [n]\). Note that here the superscripts <strong>are not exponents but are indices of coordinates, coefficients or basis vectors</strong>. We will write the summation symbol \(\Sigma\) explicitly if we do not assume Einstein’s summation convention on some index, for instance, \(\sum_{\nu=1}^m a_\nu b_\nu c^\mu d_\mu := \sum_{\mu=1}^n \sum_{\nu=1}^m a_\nu b_\nu c^\mu d_\mu\).</p> <h5 id="riemannian-manifold">Riemannian manifold</h5> <p>Riemannian geometry is used to study intrinsic properties of differentiable manifolds equipped with metrics. In machine learning, we often describe the space of some probabilistic models as a manifold. Informally, a <em>manifold</em> \(\mathcal{M}\) of dimension \(n\) is a smooth space whose local regions resemble \(\mathbb{R}^n\). Smoothness allows us to define a one-to-one local smooth mapping \(\phi: U \subset \mathcal{M} \rightarrow \mathbb{R}^n\) called <em>chart</em> or <em>coordinate system</em>, and for any \(p\in U\), \(\phi(p)\) represents the coordinates of \(p\). For instance, if \(p\) is a parameterized distribution, \(\phi(p)\) will refer to its parameters. Generally, manifolds may need a group of charts for complete description.</p> <h5 id="tangent-and-cotangent-spaces">Tangent and cotangent spaces</h5> <p>Local resemblance to \(\mathbb{R}^n\) provides the foundation for defining a linear space attached to each point \(p \in \mathcal{M}\) called the <em>tangent space</em> \(\mathcal{T}_p\mathcal{M}\). Formally, \(\mathcal{T}_p\mathcal{M}=\{ \gamma'(0): \gamma \textrm{ is a smooth curve } \gamma:\mathbb{R} \rightarrow \mathcal{M}, \gamma(0)=p \}\) is the collection of all vectors obtained by differentiating smooth curves through \(p\), and consists of all tangent vectors to the manifold at point \(p\). Each tangent vector \(v \in \mathcal{T}_p\mathcal{M}\) can be <strong>identified with a directional differential operator</strong> acting on functions defined on \(\mathcal{M}\), with direction \(v\).</p> <p>Let \(\phi(p) = (\theta^1,\theta^2,\cdots,\theta^n)\) be the coordinates of \(p\), it can be shown that the set of operators \(\{\frac{\partial}{\partial \theta^1},\cdots,\frac{\partial}{\partial \theta^n}\}\) forms a basis for \(\mathcal{T}_p\mathcal{M}\) and is called the <em>coordinate basis</em>. Note that \(\frac{\partial}{\partial \theta^\mu}\) is abbreviated to \(\partial_\mu\) and \(p\) is often identified with its coordinates \(\theta^\mu\) in the sequel.</p> <p>Any vector space \(V\) has a corresponding dual vector space \(V^*\) consisting of all <strong>linear functionals</strong> on \(V\). The dual vector space of \(\mathcal{T}_p\mathcal{M}\) is called the <em>cotangent space</em> and is denoted \(\mathcal{T}_p^* \mathcal{M}\). Because \(\mathcal{T}_p\mathcal{M}\) is finite-dimensional, \(\mathcal{T}_p^* \mathcal{M}\) has the same dimension \(n\) as \(\mathcal{T}_p\mathcal{M}\). \(\mathcal{T}_p^* \mathcal{M}\) admits the <em>dual coordinate basis</em> \(\{d\theta^1,\cdots,d\theta^n\}\). These two sets of bases satisfy \(d\theta^\mu(\partial_\nu) = \delta^\mu_\nu\) where \(\delta^\mu_\nu\) is the Kronecker delta. To gain some intuition, if \(V\) is the space of all \(n\)-dimensional column vectors, \(V^*\) can be identified with the space of \(n\)-dimensional row vectors.</p> <p>Vectors and covectors are geometric objects that exist independently of the coordinate system. Although their geometric properties do not depend on the basis used, their representation (in terms of coefficients) is of course basis-dependent. In particular, a change in the coordinate system (e.g., switching from Cartesian to polar coordinates) around \(p\) will alter the coordinate basis of \(\mathcal{T}_p\mathcal{M}\), resulting in transformations of coefficients of both vectors and covectors. Let the new chart be \(\phi'\) and let \(\phi(p) = (\theta^1,\cdots,\theta^n), \phi'(p) = (\xi^1,\cdots,\xi^n)\). The new coefficients of \(\mathbf{a} \in \mathcal{T}_p\mathcal{M}\) will be given by</p> \[a^{\mu'} \partial_{\mu'} = a^\mu \partial_\mu = a^\mu \frac{\partial \xi^{\mu'}}{\partial \theta^\mu} \partial_{\mu'} \Rightarrow a^{\mu'} = a^\mu \frac{\partial \xi^{\mu'}}{\partial \theta^\mu},\] <p>while the new coefficients of \(\mathbf{a}^* \in \mathcal{T}_p^*\mathcal{M}\) will be determined by</p> \[a_{\mu'} d\xi^{\mu'} = a_\mu d\theta^\mu = a_\mu \frac{\partial \theta^\mu}{\partial \xi^{\mu'}} d\xi^{\mu'} \Rightarrow a_{\mu'} = a_\mu \frac{\partial \theta^\mu}{\partial \xi^{\mu'}}.\] <p>Due to the difference in transformation rules, we call \(a^\mu\) to be <em>contravariant</em> while \(a_\mu\) <em>covariant</em>, as indicated by superscripts and subscripts respectively. For example, imagine a constant rescaling of the coordinates, such that all the new basis vectors are scaled by \(1/2\). To preserve the identity of a vector, its coefficients in this new basis will have to increase by a factor of \(2\) (changing contra-variantly with respect to the basis vectors). This means that the coefficients of a covector in the new basis will have to be scaled by \(1/2\) (changing co-variantly), so that it yields the same result when applied to a vector whose coefficients are twice as large in the new basis.</p> <p>Riemannian manifolds are those equipped with a positive definite metric tensor \(g_p \in \mathcal{T}_p^* \mathcal{M} \otimes \mathcal{T}_p^* \mathcal{M}\). It is a symmetric bilinear positive definite scalar function of two vectors which defines the inner product structure of \(\mathcal{T}_p\mathcal{M}\), and makes it possible to define geometric notions such as angles and lengths of curves. Namely, the inner product of two vectors \(\mathbf{a} = a^\mu \partial_\mu \in \mathcal{T}_p\mathcal{M}\), \(\mathbf{b} = b^\nu \partial_\nu \in \mathcal{T}_p\mathcal{M}\) is defined as \(\langle \mathbf{a}, \mathbf{b} \rangle := g_p(\mathbf{a}, \mathbf{b}) = g_{\mu\nu} d \theta^\mu \otimes d\theta^\nu (a^\mu \partial_\mu, b^\nu\partial_\nu) = g_{\mu\nu} a^\mu b^\nu\). Here we drop the subscript \(p\) to avoid cluttering notations, although \(g_{\mu\nu}\) can depend on \(p\). For convenience, we denote the inverse of the metric tensor as \(g^{\alpha\beta}\) using superscripts, i.e., \(g^{\alpha\beta} g_{\beta\mu} = \delta^\alpha_\mu\).</p> <p>The introduction of inner product induces a natural map from a tangent space to its dual. Let \(\mathbf{a} = a^\mu \partial_\mu \in \mathcal{T}_p\mathcal{M}\), its natural correspondence in \(\mathcal{T}_p^*\mathcal{M}\) is the covector \(\mathbf{a}^* := \langle \mathbf{a}, \cdot \rangle\). Let \(\mathbf{a}^* = a_\mu d\theta^\mu\), for any vector \(\mathbf{b} = b^\nu \partial_\nu \in \mathcal{T}_p\mathcal{M}\), we have \(\langle \mathbf{a}, \mathbf{b}\rangle = a^\mu b^\nu g_{\mu\nu} = \mathbf{a}^*(\mathbf{b}) = a_\mu d\theta^\mu(b^\nu \partial_\nu) = a_\mu b^\nu \delta^\mu_\nu = a_\mu b^\mu = a_\nu b^\nu\). Since the equation holds for arbitrary \(\mathbf{b}\), we have \(a_\nu = a^\mu g_{\mu\nu}\) and \(a^\mu = g^{\mu\nu}a_\nu\). Therefore the metric tensor relates the coefficients of a vector and its covector by lowering and raising indices.</p> <h5 id="geodesics">Geodesics</h5> <p>Consider a curve \(\gamma(t): I \rightarrow \mathbb{R}^2\). Using Cartesian coordinates \(\gamma(t)=(x(t),y(t))\), it is easy to verify that \(\gamma\) is a straight line if it satisfies the following equation \(\frac{d^2 x}{d t} =\frac{d^2 y}{d t}=0\), i.e., \(x(t) = x_0 + v_x t, y(t) = y_0 + v_y t\). However, if we represent the curve using polar coordinates \(\gamma(t)=(r(t),\theta(t))\), i.e., \(x = r \cos \theta, y = r \sin \theta\), then \(\frac{d^2 r}{d t} =\frac{d^2 \theta}{d t}=0\) does <em>not</em> correspond to a straight line. The issue is that the coordinate basis depends on the position and will change as we move. For example, the radial directions \(\partial r\) at two points on a circle will be different.</p> <p>We denote \(\nabla_{\mu} \partial_\nu\) as the vector describing how fast the basis \(\partial_\nu\) changes in the direction of \(\partial_\mu\). It is given by \(\nabla_{\mu} \partial_\nu = \Gamma_{\mu\nu}^\alpha \partial_\alpha\) where \(\Gamma_{\mu\nu}^\alpha\) is the <em>connection</em>, which specifies additional manifold structure associated with curvature and parallelness. Here we use the Levi-Civita connection which is defined in terms of the metric as</p> \[\Gamma_{\mu\nu}^\alpha = \frac{1}{2} g^{\alpha \beta}(\partial_\mu g_{\beta\nu} + \partial_\nu g_{\beta\mu} - \partial_\beta g_{\mu\nu}).\] <p>A connection also specifies a way to transport a vector along a curve so that it stays parallel (to itself) with respect to the connection. Given the change rate of bases specified by the connection, the change rate of any vector \(\mathbf{a}\) along direction \(\mathbf{b}\) can be computed by</p> \[\nabla_{\mathbf{b}} \mathbf{a} = b^\mu \nabla_\mu (a^\nu \partial_\nu) = b^\mu (\partial_\mu a^\nu) \partial_\nu + b^\mu a^\nu \Gamma_{\mu\nu}^\alpha \partial_\alpha.\] <p>We define the <em>parallel transport</em> of a vector \(\mathbf{a}\) along a curve \(\theta(t)\) as the solution to \(\nabla_{\dot \theta} \mathbf{a}(t) = 0\), where \(\dot \theta = \frac{d\theta}{dt}\) represents the tangent vector of \(\theta(t)\). This notion can be used to define the analogue of “straight lines’’, or geodesics, in general manifolds. A curve \(\theta(t)\) is a geodesic if the tangent vector \(\dot \theta = \frac{d\theta}{dt}\) is propagated parallelly to itself along the curve. This gives the geodesic equation \(\nabla_{\dot{\theta}} \dot{\theta} = 0\), or equivalently</p> \[\ddot\theta^\alpha + \Gamma_{\mu\nu}^\alpha \dot\theta^\mu \dot \theta^\nu = 0\] <p>Note that \(\Gamma_{\mu\nu}^{\alpha}=0\) for \(\mathbb{R}^n\) with Euclidean metric and we recover usual straight lines. Let \(v \in \mathcal{T}_p\mathcal{M}\), there exists a unique geodesic satisfying \(\gamma(0)=p, \dot{\gamma}(0) = v\). This enables us to define the <em>exponential map</em>, i.e., \(\operatorname{Exp}(p,v) := \gamma(1)\). By simple scaling we also have \(\operatorname{Exp}(p, h v) = \gamma(h)\). Consistent with our intuition, geodesics (w.r.t. Levi-Civia connection) also correspond to local shortest paths between two points.</p> <h5 id="summary">Summary</h5> <p>As a summary, we provide a graphical illustration of relevant concepts in Riemannian geometry.</p> <p><img src="/assets/img/geo/tangent_plane_plot.jpg" alt="" style="width: 100%;"/></p> <p>From the above formulation, we observe that every concept and relation is defined via coordinates or coefficients. However, we need to discern the important ontological difference between an object and its coordinate. The manifold itself, along with geodesics and vectors (covectors) in its tangent (cotangent) spaces is <strong>intrinsic and independent of coordinates</strong> provided by charts. Riemannian geometry studies intrinsic properties of manifolds via the lens of coordinates. As long as the coordinates are transformed accordingly, e.g., both transforms covariantly or contravariantly, the objects and their relations will not change. This is where invariance emerges.</p> <h5 id="relation-to-natural-gradient-method">Relation to natural gradient method</h5> <p>Let \(r_\theta(\mathbf{z}) = p_\theta(\mathbf{t}\mid\mathbf{x})q(\mathbf{x})\) be a probabilistic model parameterized by \(\theta \in \Theta\), and \(L(r_\theta) = - \mathbb{E}_{\mathbf{x}\sim q} [\log p_\theta(\mathbf{t}\mid\mathbf{x})]\) be the expected negative log-likelihood. Here \(\mathbf{x},\mathbf{t}\) are random variables, \(\mathbf{z} = (\mathbf{x},\mathbf{t})\) is their joint, \(q(\mathbf{x})\) is the marginal distribution of \(\mathbf{x}\) and is independent of \(\theta\). In a Riemannian geometry framework, the set of all possible models \(r_\theta\) constitutes a manifold \(\mathcal{M}\), and the parameter \(\theta\) provides a chart. Our goal is to find a model \(r_{\theta^*}\) that minimizes the (empirical) loss \(L(r_\theta)\), which is a function defined on the manifold.</p> <p>The well known update rule of gradient descent \(\theta_{k+1}^\mu = \theta_k^\mu - h \lambda \partial_\mu L(r_{\theta_k})\) can be viewed as approximately solving the ordinary differential equation (ODE) \(\dot\theta^\mu = -\lambda \partial_\mu L(r_\theta)\) with forward Euler method. Here \(\lambda\) is a time scale constant, \(h\) is the step size, and their product \(h\lambda\) is the learning rate. However, the gradient descent ODE is not invariant to reparameterizations. For example, if we rescale \(\theta^\mu\) to \(2 \theta^\mu\), \(\partial_\mu L(r_\theta)\) will be downscaled to \(\frac{1}{2} \partial_\mu L(r_\theta)\). This is more evident from a differential geometric point of view. As can be verified by chain rules, \(\dot\theta^\mu\) transforms contravariantly and therefore is a vector in \(\mathcal{T}_p\mathcal{M}\), while \(\partial_\mu L(r_\theta)\) transforms covariantly, thus being a covector in \(\mathcal{T}^*_p\mathcal{M}\). Accordingly, the l.h.s. and r.h.s. of the ODE transform with different rules and <strong>do not type check</strong>. As a result, optimizing \(L(r_\theta)\) with gradient descent is using a parameterization-dependent method to solve a parameterization-independent problem, which is aesthetically unsatisfying.</p> <p>Natural gradient alleviates this issue by approximately solving an invariant ODE. Recall that we can raise or lower an index given a metric tensor \(g_{\mu\nu}\), and hereby we choose the Fisher information metric given by</p> \[g_{\mu\nu} = \mathbb{E}_{\mathbf{x}\sim q} \mathbb{E}_{p_\theta(\mathbf{t}\mid\mathbf{x})}[\partial_\mu\log p_\theta(\mathbf{t}\mid\mathbf{x})\partial_\nu \log p_\theta(\mathbf{t}\mid\mathbf{x})].\] <p>By raising the index of \(\partial_\mu L(r_\theta)\), the r.h.s. of the gradient descent ODE becomes a vector in \(\mathcal{T}_p\mathcal{M}\) and both sides transform contravariantly. The new invariant ODE is now \(\dot\theta^\mu = - \lambda g^{\mu\nu} \partial_\nu L(r_\theta)\), and the forward Euler approximation becomes \(\theta_{k+1}^\mu = \theta_k^\mu - h \lambda g^{\mu\nu}\partial_\nu L(r_{\theta_k})\), which is the traditional natural gradient update.</p> </details> <hr/> <p>Intuitively, natural gradient is not the gradient with respect to model parameters; rather, it is the gradient on the manifold of probabilistic models. As opposed to the traditional gradient which considers how perturbations of the model parameters affect the loss function, natural gradient considers how the loss function changes when the probabilistic model moves a little bit on the manifold of our model family. Because the model family does not change with respect to parameterizations, the natural gradient will also remain the same. This means that the form of the natural gradient will transform appropriately between parameterizations, automatically ensuring that the natural gradient under two different parameterizations correspond to the same “gradient entity” on the manifold.</p> <p>Mathematically, suppose the loss function is \(L(p_\theta(x,y))\), we can write the natural gradient as</p> \[\widetilde{\nabla}_\theta L(p_\theta(x,y)) := \mathbf{F}^{-1}_\theta \nabla_\theta L(p_\theta(x, y)),\] <p>where \(\mathbf{F}_\theta\) is the <a href="https://en.wikipedia.org/wiki/Fisher_information#Matrix_form">Fisher information matrix</a> for \(p_\theta(x, y)\), i.e.,</p> \[[\mathbf{F}_\theta]_{i,j} := \mathbb{E}_{p_\theta(x, y)} \left[\left( \frac{\partial}{\partial \theta_i} \log p_\theta(x,y)\right) \left( \frac{\partial}{\partial \theta_j} \log p_\theta(x,y) \right) \right],\] <p>and we use a tilde mark (~) to differentiate the notation of natural gradient from that of ordinary gradient.</p> <p>As a sanity check, let’s see how natural gradient eliminates the non-invariance of gradient descent for our linear regression example. For different parameterizations of Gaussian conditional distributions, the derivatives of the log densities are respectively</p> \[\begin{aligned} \frac{\partial}{\partial \theta} \log p_\theta(x,y) &amp;= \frac{(y-\theta x- b) x}{\sigma^2}\\ \frac{\partial}{\partial \mu} \log p_\mu(x,y) &amp;= \frac{2(y - 2\mu x -b) x}{\sigma^2}\\ \end{aligned}\] <p>Therefore, suppose \(\theta_t = 2\mu_t = a\), we have \(\frac{\partial}{\partial \mu} \log p_\mu(x,y) \vert_{\mu=\mu_t} = 2 \frac{\partial}{\partial \theta} \log p_\theta(x,y)\vert_{\theta=\theta_t}\) and therefore \(\mathbf{F}_{\mu=\mu_t} = 4\mathbf{F}_{\theta=\theta_t}\). From our previous analysis on gradient descent, we already know that \(\nabla_\mu L(p_\mu(x, y))\vert_{\mu=\mu_t} = 2 \nabla_\theta L(p_\theta(x, y))\vert_{\theta_t=a}\). As a result, the natural gradient scales as</p> \[\widetilde{\nabla}_\mu L(p_\mu(x,y))\vert_{\mu=\mu_t} = \frac{1}{2}\widetilde{\nabla}_\theta L(p_\theta(x,y))\vert_{\theta=\theta_t},\] <p>which indicates invariance of the optimization step:</p> \[\begin{aligned} \theta_{t+1} &amp;= \theta_{t} - \alpha \widetilde{\nabla}_{\theta_t} L(p_{\theta_t}(x,y))\\ &amp;= 2 \mu_t - 2 \alpha \widetilde{\nabla}_{\mu_t} L(p_{\mu_t}(x,y))\\ &amp;= 2\mu_{t+1}. \end{aligned}\] <p>More generally, consider an <a href="https://en.wikipedia.org/wiki/Ordinary_differential_equation">ordinary differential equation (ODE)</a> that describes how the parameter \(\theta\) should change as a function of \(t\), following the natural gradient smoothly:</p> \[\frac{d \theta}{d t} = - \lambda \widetilde{\nabla}_\theta L(p_\theta(x,y))\] <p>Here \(\lambda\) is just a prespecified scaling constant. If we change the parameterization from \(\theta\) to \(\mu\), we get a similar ODE:</p> \[\frac{d \mu}{d t} = - \lambda \widetilde{\nabla}_\mu L(p_\mu(x,y)).\] <p>The following observation on invariance can be derived with knowledge of information theory and Riemannian geometry:</p> <blockquote> <p>Suppose \(\mu\) is a smoothly differentiable transformation of \(\theta\). If \(p_{\theta(t=0)}(x,y) \equiv p_{\mu(t=0)}(x,y)\), and \(\theta(t)\), \(\mu(t)\) are solutions of corresponding natural gradient ODEs, then we have \(p_{\theta(t=a)}(x,y) \equiv p_{\mu(t=a)}(x,y)\) for any \(a \geq 0\).</p> </blockquote> <p>However, it is usually intractable to solve such a complicated ODE for practical optimization tasks, and various approximation schemes must be used. From the perspective of numerical methods for ODEs, the naive natural gradient descent method</p> \[\theta_{t+1} = \theta_{t} - h \lambda \widetilde{\nabla}_{\theta_t} L(p_{\theta_t}(x,y))\] <p>can be viewed as approximately solving the ODE</p> \[\frac{d \theta}{d t} = - \lambda \widetilde{\nabla}_\theta L(p_\theta(x,y))\] <p>using the <a href="https://en.wikipedia.org/wiki/Euler_method">forward Euler method</a> with a step size of \(h\). <strong>Such an approximation with discrete step size will potentially cause loss of invariance</strong>.</p> <p>The following picture illustrates this point more clearly.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/geo/curved_path.jpg"/> </div> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/geo/straight_path_1.jpg"/> </div> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/geo/straight_path_2.jpg"/> </div> </div> <p style="color: #666666; font-size: 12px; text-align: center; line-height: 1.2">An illustration of optimization trajectories on the manifold (left), parameter space \(\Theta\) (middle) and another parameter space \(\Xi\) (right). The <em>red</em> dotted arrow represents a truly invariant trajectory, while the light and dark <em>blue</em> solid arrows represent trajectories of vanilla natural gradient updates under different parameterizations.</p> <p>Suppose we are doing optimization on a manifold of probabilistic models \(\mathcal{M}\), and consider two different parameterizations with parameter spaces denoted as \(\Theta\) and \(\Xi\) respectively. The red arrow shows the invariant optimization trajectory obtained by accurately solving the natural gradient ODE. The light and dark blue arrows depict the optimization trajectory of the vanilla natural gradient descent under two different parameterizations.For the red arrows, we show the trajectory on manifold \(\mathcal{M}\), as well as trajectories in parameter space \(\Theta\) and \(\Xi\). For the blue arrows, we only show trajectories on \(\mathcal{M}\) and the parameter space.</p> <p>Because vanilla natural gradient descent is a linear update, the blue trajectories in parameter space \(\Theta\) and \(\Xi\) are straight lines. However, in order to obtain the invariant trajectory (red arrow), the update has to be curved in parameter space. The update of the vanilla natural gradient descent is straight in the parameter space, and therefore its trajectory will not be truly invariant on \(\mathcal{M}\). For more invariant solutions, we need more complicated update rules that can perform curved updates in parameter space as well.</p> <p>Because the natural gradient ODE solution is invariant, a straightforward approach to improving the invariance of the optimization is to exploit more accurate solvers. In numerical analysis, the accuracy of a solver is measured by computing its convergence order to the exact solution when step size \(h \rightarrow 0\). We say that a numerical ODE solver is \(d\)-th order accurate if the error between exact and approximate solutions decreases as \(O(h^d)\). The forward Euler solver used in the traditional natural gradient update is a simple first-order solver.</p> <p>In a similar way, we can also measure the “invariance order” of a numerical ODE solver. We say that a solver is \(d\)-th order invariant if the error between the approximate solution and <strong>some exactly invariant trajectory</strong> decreases as \(O(h^d)\). Because the exact solution of the natural gradient ODE is invariant, any \(d\)-th order accurate solver is also \(d\)-th order invariant. We therefore have two ways to improve the invariance of natural gradient update — (1) using higher-order accurate solvers for the natural gradient ODE, or (2) using more invariant (but not necessarily more accurate) solvers.</p> <h3 id="numerical-ode-solvers-that-achieve-better-invariance">Numerical ODE solvers that achieve better invariance</h3> <p>We discuss two approaches to improve the invariance of the natural gradient update:</p> <h4 id="1-more-accurate-solvers">1. More accurate solvers</h4> <p>Natural gradient ODE has an invariant solution; we just need to improve the accuracy of our numerical ODE solver such that the optimization trajectory is closer to the invariant one. In the paper, we discussed using a second-order solver called the <a href="https://en.wikipedia.org/wiki/Midpoint_method">Midpoint Method</a> as a replacement for the first-order forward Euler method.</p> <p>The Midpoint method can be decomposed into three steps to approximately solve the natural gradient ODE:</p> <ol> <li>Use gradient descent with half the step size to compute the midpoint: \(\theta_{t+1/2} = \theta_{t} - \frac{1}{2} h \lambda \widetilde{\nabla}_{\theta_t} L(p_{\theta_t}(x,y))\)</li> <li>Compute the gradient at the midpoint: \(\widetilde{\nabla}_{\theta_{t + 1/2}} L(p_{\theta_{t+1/2}}(x, y))\)</li> <li>Do the final update with the midpoint gradient: \(\theta_{t + 1} = \theta_{t} - h \lambda \widetilde{\nabla}_{\theta_{t+1/2}} L(p_{\theta_{t+1/2}}(x,y))\)</li> </ol> <h4 id="2-more-invariant-solvers">2. More invariant solvers</h4> <p>Some solvers can be more invariant by making their update rules more agonistic to reparameterizations, even though they are not necessarily more accurate than the forward Euler method. In the paper, we investigated a first-order solver called the Riemannian Euler Method <d-cite key="bielecki2002estimation"></d-cite> which is <em>exactly invariant</em> to reparameterizations when approximately solving the natural gradient ODE. In other words, when starting from the same initial probabilistic model, the Riemannian Euler method will return the same final probabilistic model after a fixed number of iterations, as long as the learning rate is fixed.</p> <p>When applied to solving the natural gradient ODE, the Riemannian Euler method uses the following update rule:</p> \[\theta_{t+1} = \operatorname{Exp}(\theta_t, -h \lambda \widetilde{\nabla}_{\theta_t} L(p_{\theta_t}(x, y))),\] <p>where \(\operatorname{Exp}(\cdot, \cdot)\) is the <a href="https://en.wikipedia.org/wiki/Exponential_map_(Riemannian_geometry)">Exponential Map</a>, a concept related to geodesics in Riemannian geometry. Unfortunately, evaluating \(\operatorname{Exp}(\cdot, \cdot)\) requires calculating geodesics on the manifold of probabilistic models, which is usually intractable.</p> <p>The good news is that this exponential map can be easier to approximate than the invariant solution of the natural gradient ODE. Leveraging the <a href="https://en.wikipedia.org/wiki/Solving_the_geodesic_equations">geodesic equation</a>—an ODE governing geodesic lines on a manifold—we can obtain a <strong>second-order</strong> approximation to the exponential map. Equipped with this approximation, the update rule now becomes</p> \[\begin{aligned} \gamma_t &amp;= -\lambda \widetilde{\nabla}_{\theta_t} L(p_{\theta_t}(x,y))\\ \theta_{t+1} &amp;= \underbrace{\theta_{t} + h \gamma_t}_{\text{naive natural gradient update}} \underbrace{- \frac{1}{2}h^2 \Gamma_{\theta_t}(\gamma_t,\gamma_t)}_{\text{geodesic correction}}. \end{aligned}\] <p>Here \(\Gamma_{\theta_t}(\cdot, \cdot)\) represents the result of applying the <a href="https://en.wikipedia.org/wiki/Levi-Civita_connection">Levi-Civita connection</a><sup id="fnref:connection"><a href="#fn:connection" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> to input vectors. We name this update rule <em>natural gradient update with geodesic correction</em>.</p> <p>An interesting observation is that if we use the naive natural gradient update rule, it amounts to using the Riemannian Euler method with <strong>first-order</strong> approximation to \(\operatorname{Exp}(\cdot, \cdot)\). This observation confirms that geodesic correction can provide more invariance (asymptotically).</p> <p>The new numerical ODE solvers we explored so far are all computationally more expensive than the naive natural gradient update. For the midpoint integrator, we must calculate the midpoint first. For geodesic correction, we need to compute the Levi-Civita connection term. Both methods require about twice the computational cost compared to the naive natural gradient update. Luckily, we discovered that there exists a <strong>faster</strong> method for doing geodesic correction which still ensures a second-order approximation to the exponential map, presevering invariance. This faster geodesic correction is roughly <strong>as computationally expensive as</strong> the naive natural gradient update. Please refer to <a href="https://arxiv.org/abs/1803.01273">our paper</a> for more details.</p> <h3 id="overview-of-experimental-results">Overview of experimental results</h3> <p>Finally, we apply our techniques to improve the invariance of the natural gradient update and observe how they affect optimization performance. We first examine a toy problem to demonstrate that our methods achieve better invariance, then show that they lead to improved optimization for several tasks in various areas of machine learning, including supervised/unsupervised learning and reinforcement learning.</p> <h4 id="testing-invariance-on-a-toy-problem">Testing invariance on a toy problem</h4> <p>We experiment with different natural gradient updates to fit a univariate <a href="https://en.wikipedia.org/wiki/Gamma_distribution">Gamma distribution</a> under different parameterizations. The canonical parameterization of a Gamma distribution is</p> \[p(x \mid \alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha - 1} e^{-\beta x},\] <p>where \(\Gamma(\cdot)\) denotes the <a href="https://en.wikipedia.org/wiki/Gamma_function">Gamma function</a>.</p> <p>Aside from this canonical parameterization, we test three other parameterizations:</p> <ol> <li>Replacing \(\alpha\), \(\beta\) with \(\alpha'\), \(\frac{1}{\beta'}\).</li> <li>Replacing \(\alpha, \beta\) with \(\alpha', (\beta')^3\).</li> <li>Replacing \(\alpha, \beta\) with \((\alpha')^2, (\beta')^2\).</li> </ol> <p>Next, we test all the natural gradient updates to see how well they can fit the parameters of a univariate Gamma distribution under different parameterizations. In the following figure, we show how the negative log-likelihood (our training loss) decreases with respect to training iterations. We use <strong>ng</strong> to denote the naive natural gradient update, <strong>mid</strong> for midpoint integrator, <strong>geo</strong> for geodesic correction, <strong>geo<sub>f</sub></strong> for the faster version of geodesic correction, <strong>ng(exact)</strong> for solving the natural gradient ODE exactly, and <strong>geo(exact)</strong> for using the exact Riemannian Euler method (without approximating the exponential map).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/geo/gamma.png"/> </div> </div> <p><br/></p> <p>As the theory predicts, the curves of <strong>ng(exact)</strong> and <strong>geo(exact)</strong> are the same for all four different parameterizations. All of our proposed methods—<strong>mid</strong>, <strong>geo</strong> and <strong>geo<sub>f</sub></strong>—are closer to the exact invariant optimization curves. In stark contrast, the naive natural gradient update leads to different curves under different parameterizations, and yield slower convergence.</p> <h4 id="fitting-deep-auto-encoders-and-deep-classifiers">Fitting deep auto-encoders and deep classifiers</h4> <p>We then show the results of training deep autoencoders and classifiers on the MNIST dataset<sup id="fnref:mnist"><a href="#fn:mnist" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. The following figure illustrates how the different optimization algorithms fare against each other. The bottom x-axis corresponds to the number of iterations during training, while the top x-axis corresponds to the wall-clock time. The y-axis shows the training error. Each optimization algorithm has two curves with the same color: one is solid while the other is dashed. We can observe that improved natural gradient updates converge faster in terms of number of iterations, and the faster geodesic correction also achieves lower loss in shorter wall-clock time, compared to the naive update.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/geo/fig.jpg"/> </div> </div> <p><br/></p> <h4 id="model-free-reinforcement-learning-for-continuous-control">Model-free reinforcement learning for continuous control</h4> <p>Finally, we show that our techniques also improve the performance of natural gradient-based optimization algorithms in reinforcement learning. In the experiments, we apply our techniques to ACKTR <d-cite key="wu2017scalable"></d-cite>, a natural gradient algorithm for policy optimization in reforcement learning. The results on the <a href="https://gym.openai.com/envs/HalfCheetah-v2/">HalfCheetah</a><sup id="fnref:rl"><a href="#fn:rl" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> environment are shown in the following figure, where we obtain higher rewards faster after combining our techniques with ACKTR.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/img/geo/acktr.jpg" style="width:50%; display: block; margin-left: auto; margin-right: auto; text-align: center;'"/> </div> </div> <h3 id="conclusion">Conclusion</h3> <p>In this work, we analyze the invariance of optimization algorithms. Based on our analysis, we propose several methods to improve the invariance of natural gradient update. We empirically report that being more invariant leads to faster optimization for many machine learning tasks.</p> <h4 id="footnotes">Footnotes</h4> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:connection"> <p>This is again a Riemannian geometry concept. The reader only needs to know that in practice \(\Gamma_{\theta_t}(\cdot, \cdot)\) can be computed almost as fast as \(\widetilde{\nabla}_{\theta_t} L(p_{\theta_t}(x,y))\). Please refer to the paper for more information. <a href="#fnref:connection" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:mnist"> <p>We also have results on 2 other datasets in the paper. <a href="#fnref:mnist" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:rl"> <p>We tested 5 other environments in the paper. <a href="#fnref:rl" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>Yang Song</name></author><summary type="html"><![CDATA[An overview for our ICML 2018 paper, Accelerating Natural Gradient with Higher-Order Invariance. Natural gradient update loses its invariance due to the finite step size. In this paper, we study the invariance of natural gradient from the perspective of Riemannian geometry, and propose several new update rules to improve its invariance. Empirical results show that better invariance can result in faster convergence in several supervised learning, unsupervised learning and reinforcement learning applications.]]></summary></entry><entry><title type="html">a post with comments</title><link href="https://yang-song.net/blog/2015/comments/" rel="alternate" type="text/html" title="a post with comments"/><published>2015-10-20T15:59:00+00:00</published><updated>2015-10-20T15:59:00+00:00</updated><id>https://yang-song.net/blog/2015/comments</id><content type="html" xml:base="https://yang-song.net/blog/2015/comments/"><![CDATA[<p>This post shows how to add DISQUS comments.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="external-services"/><summary type="html"><![CDATA[an example of a blog post with comments]]></summary></entry><entry><title type="html">a post with math</title><link href="https://yang-song.net/blog/2015/math/" rel="alternate" type="text/html" title="a post with math"/><published>2015-10-20T15:12:00+00:00</published><updated>2015-10-20T15:12:00+00:00</updated><id>https://yang-song.net/blog/2015/math</id><content type="html" xml:base="https://yang-song.net/blog/2015/math/"><![CDATA[<p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\sum_{k=1}^\infty |\langle x, e_k \rangle|^2 \leq \|x\|^2\] <p>You can also use <code class="language-plaintext highlighter-rouge">\begin{equation}...\end{equation}</code> instead of <code class="language-plaintext highlighter-rouge">$$</code> for display mode math. MathJax will automatically number equations:</p> <p>\begin{equation} \label{eq:cauchy-schwarz} \left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right) \end{equation}</p> <p>and by adding <code class="language-plaintext highlighter-rouge">\label{...}</code> inside the equation environment, we can now refer to the equation using <code class="language-plaintext highlighter-rouge">\eqref</code>.</p> <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="math"/><summary type="html"><![CDATA[an example of a blog post with some math]]></summary></entry><entry><title type="html">a post with code</title><link href="https://yang-song.net/blog/2015/code/" rel="alternate" type="text/html" title="a post with code"/><published>2015-07-15T15:09:00+00:00</published><updated>2015-07-15T15:09:00+00:00</updated><id>https://yang-song.net/blog/2015/code</id><content type="html" xml:base="https://yang-song.net/blog/2015/code/"><![CDATA[<p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. Produces something like this:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="code"><pre><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
    <span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>
    
    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>
    
    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></figure>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[an example of a blog post with some code]]></summary></entry><entry><title type="html">a post with images</title><link href="https://yang-song.net/blog/2015/images/" rel="alternate" type="text/html" title="a post with images"/><published>2015-05-15T21:01:00+00:00</published><updated>2015-05-15T21:01:00+00:00</updated><id>https://yang-song.net/blog/2015/images</id><content type="html" xml:base="https://yang-song.net/blog/2015/images/"><![CDATA[<p>This is an example post with image galleries.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/9-1400.webp"/> <img src="/assets/img/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/7-1400.webp"/> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <p>Images can be made zoomable. Simply add <code class="language-plaintext highlighter-rouge">data-zoomable</code> to <code class="language-plaintext highlighter-rouge">&lt;img&gt;</code> tags that you want to make zoomable.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/8-1400.webp"/> <img src="/assets/img/8.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/10-1400.webp"/> <img src="/assets/img/10.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The rest of the images in this post are all zoomable, arranged into different mini-galleries.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/11-1400.webp"/> <img src="/assets/img/11.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/12-1400.webp"/> <img src="/assets/img/12.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/7-1400.webp"/> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included images could look like]]></summary></entry></feed>